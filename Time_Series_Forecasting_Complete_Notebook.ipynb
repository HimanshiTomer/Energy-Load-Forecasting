{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0q2IAzxoHkgwNWEDUxl37"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#REQUIRED INSTALLs"
      ],
      "metadata": {
        "id": "tyOKMPfXLlmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EevXD2eqEliM",
        "outputId": "6951e172-1701-4aaa-8f9e-6d732c3baaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/689.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m686.1/689.1 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pmdarima statsmodels --quiet\n",
        "!pip install openpyxl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORTS"
      ],
      "metadata": {
        "id": "bKEu8f_-LpWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from pmdarima import auto_arima\n",
        "from itertools import product\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "YEnXUgkfE5Xu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All packages installed and imported successfully!\")\n",
        "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "adZLziepE9Jh",
        "outputId": "f46f5340-25cf-4dfe-fbd6-1146d133b8de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "All packages installed and imported successfully!\n",
            "Execution started at: 2025-11-20 17:57:56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DRIVE PATHS AND FOLDER STRUCTURE"
      ],
      "metadata": {
        "id": "Iv53GG2uLwkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base path - UPDATE THIS PATH TO YOUR DRIVE LOCATION\n",
        "BASE_PATH = '/content/drive/MyDrive/Time_Series_Project'\n",
        "\n",
        "# Create folder structure\n",
        "FOLDERS = {\n",
        "    'data_raw': f'{BASE_PATH}/data/raw',\n",
        "    'data_cleaned': f'{BASE_PATH}/data/cleaned',\n",
        "    'models': f'{BASE_PATH}/models',\n",
        "    'results_viz': f'{BASE_PATH}/results/visualizations',\n",
        "    'results_eval': f'{BASE_PATH}/results/evaluations',\n",
        "    'results_forecast': f'{BASE_PATH}/results/forecasts',\n",
        "    'reports': f'{BASE_PATH}/reports'\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "for folder_name, folder_path in FOLDERS.items():\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    print(f\"Created/Verified: {folder_name}\")\n",
        "\n",
        "\n",
        "print(\"FOLDER STRUCTURE READY\")\n",
        "\n",
        "print(\"Please upload the following files to the 'data/raw' folder:\")\n",
        "print(\"1. electricity_requirement_mu_2015_2025.csv\")\n",
        "print(\"2. monthly_festival_index_detailed_2015_2025.csv\")\n",
        "print(\"3. temperature_india_monthly_2015_2025.csv\")\n",
        "print(\"4. iip_yoy_2014_2025.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IPqJi3mwE91c",
        "outputId": "52e58993-a7e7-441a-9f83-c6e0ccd99a20"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created/Verified: data_raw\n",
            "Created/Verified: data_cleaned\n",
            "Created/Verified: models\n",
            "Created/Verified: results_viz\n",
            "Created/Verified: results_eval\n",
            "Created/Verified: results_forecast\n",
            "Created/Verified: reports\n",
            "FOLDER STRUCTURE READY\n",
            "Please upload the following files to the 'data/raw' folder:\n",
            "1. electricity_requirement_mu_2015_2025.csv\n",
            "2. monthly_festival_index_detailed_2015_2025.csv\n",
            "3. temperature_india_monthly_2015_2025.csv\n",
            "4. iip_yoy_2014_2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOADING AND INITIAL EXPLORATION"
      ],
      "metadata": {
        "id": "pO5fBdvKL5Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LOADING DATASETS\")\n",
        "\n",
        "# Load datasets\n",
        "electricity_df = pd.read_csv(f\"{FOLDERS['data_raw']}/electricity_requirement_mu_2015_2025.csv\")\n",
        "festival_df = pd.read_csv(f\"{FOLDERS['data_raw']}/monthly_festival_index_detailed_2015_2025.csv\")\n",
        "temperature_df = pd.read_csv(f\"{FOLDERS['data_raw']}/temperature_india_monthly_2015_2025.csv\")\n",
        "iip_df = pd.read_csv(f\"{FOLDERS['data_raw']}/iip_yoy_2014_2025.csv\")\n",
        "\n",
        "print(\"All datasets loaded successfully!\")\n",
        "print(f\"\\nElectricity data shape: {electricity_df.shape}\")\n",
        "print(f\"Festival data shape: {festival_df.shape}\")\n",
        "print(f\"Temperature data shape: {temperature_df.shape}\")\n",
        "print(f\"IIP data shape: {iip_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eWtIQ-S4FBcu",
        "outputId": "8f612817-da68-4bf7-9a4f-6728b58641af"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING DATASETS\n",
            "All datasets loaded successfully!\n",
            "\n",
            "Electricity data shape: (131, 4)\n",
            "Festival data shape: (128, 8)\n",
            "Temperature data shape: (128, 4)\n",
            "IIP data shape: (145, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA CLEANING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "ne2Tc4dbFGMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DATA CLEANING AND PREPROCESSING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QgcJv_eIFERy",
        "outputId": "175e51d3-69ee-4286-a5d1-9a526cb248e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA CLEANING AND PREPROCESSING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Electricity Data"
      ],
      "metadata": {
        "id": "6NQJhPArMFIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[1/4] Cleaning Electricity Data...\")\n",
        "# Remove footer rows (copyright, NaN rows)\n",
        "electricity_clean = electricity_df[electricity_df['Parameter'].notna()].copy()\n",
        "electricity_clean = electricity_clean[electricity_clean['Parameter'].str.contains(r'\\d{4}-\\d{2}', na=False)]\n",
        "\n",
        "# Rename columns\n",
        "electricity_clean.columns = ['Date', 'Requirement_MU', 'Supplied_MU', 'Shortage_Percent']\n",
        "\n",
        "# Convert Date to datetime\n",
        "electricity_clean['Date'] = pd.to_datetime(electricity_clean['Date'], format='%Y-%m')\n",
        "\n",
        "# Extract Year and Month\n",
        "electricity_clean['Year'] = electricity_clean['Date'].dt.year\n",
        "electricity_clean['Month'] = electricity_clean['Date'].dt.month\n",
        "\n",
        "# Reset index\n",
        "electricity_clean = electricity_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"   Cleaned shape: {electricity_clean.shape}\")\n",
        "print(f\"   Date range: {electricity_clean['Date'].min()} to {electricity_clean['Date'].max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wAdk17G9FGfD",
        "outputId": "4d0e877e-50e7-4cec-e765-2be84256a396"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/4] Cleaning Electricity Data...\n",
            "   Cleaned shape: (125, 6)\n",
            "   Date range: 2015-04-01 00:00:00 to 2025-08-01 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Festuval Data"
      ],
      "metadata": {
        "id": "AqfmZbFZFN3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[2/4] Cleaning Festival Data...\")\n",
        "\n",
        "festival_clean = festival_df[['Year', 'Month', 'Monthly_Festival_Index']].copy()\n",
        "print(f\"   Cleaned shape: {festival_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0UPcKuSMFOST",
        "outputId": "400940ec-156b-472f-ac41-dec3402f8f0f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/4] Cleaning Festival Data...\n",
            "   Cleaned shape: (128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature-Data"
      ],
      "metadata": {
        "id": "2obUPkNtFRVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[3/4] Cleaning Temperature Data...\")\n",
        "\n",
        "# Convert month to datetime\n",
        "temperature_clean = temperature_df.copy()\n",
        "temperature_clean['Date'] = pd.to_datetime(temperature_clean['month'], format='%Y-%m')\n",
        "temperature_clean['Year'] = temperature_clean['Date'].dt.year\n",
        "temperature_clean['Month'] = temperature_clean['Date'].dt.month\n",
        "\n",
        "# Drop NaN rows\n",
        "temperature_clean = temperature_clean.dropna(subset=['mean'])\n",
        "\n",
        "# Select relevant columns\n",
        "temperature_clean = temperature_clean[['Year', 'Month', 'mean']].copy()\n",
        "temperature_clean.columns = ['Year', 'Month', 'Temperature_Mean']\n",
        "\n",
        "print(f\"   Cleaned shape: {temperature_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2UEpNkLzFRf1",
        "outputId": "d9d32a1e-18c6-4549-af78-617970202c16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/4] Cleaning Temperature Data...\n",
            "   Cleaned shape: (127, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IIP YoY% data"
      ],
      "metadata": {
        "id": "iA7iwTLdFVjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[4/4] Cleaning IIP YoY% Data...\")\n",
        "\n",
        "# Remove NaN rows\n",
        "iip_clean = iip_df[iip_df['month'].notna()].copy()\n",
        "\n",
        "# Remove extra columns\n",
        "iip_clean = iip_clean[['month', 'iip_yoy']].copy()\n",
        "\n",
        "# Correct typo 'Februrary' to 'February'\n",
        "iip_clean['month'] = iip_clean['month'].str.replace('Februrary', 'February', regex=False)\n",
        "\n",
        "# Parse the month column (format: \"January, 2015\")\n",
        "# Use format='mixed' to handle potential inconsistencies or typos in month names like 'Februrary'\n",
        "iip_clean['Date'] = pd.to_datetime(iip_clean['month'], format='mixed', dayfirst=False)\n",
        "iip_clean['Year'] = iip_clean['Date'].dt.year\n",
        "iip_clean['Month'] = iip_clean['Date'].dt.month\n",
        "\n",
        "# Select relevant columns\n",
        "iip_clean = iip_clean[['Year', 'Month', 'iip_yoy']].copy()\n",
        "iip_clean.columns = ['Year', 'Month', 'IIP_YoY']\n",
        "\n",
        "print(f\"   Cleaned shape: {iip_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "foDMKw8IFV8g",
        "outputId": "d2c9f19c-a89d-482e-f3cc-ae9960d527e1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/4] Cleaning IIP YoY% Data...\n",
            "   Cleaned shape: (132, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MERGING ALL DATASETS"
      ],
      "metadata": {
        "id": "jIOjXWYIFYdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MERGING DATASETS\")\n",
        "\n",
        "# Start with electricity data (our target variable)\n",
        "merged_df = electricity_clean[['Date', 'Year', 'Month', 'Requirement_MU']].copy()\n",
        "\n",
        "# Merge festival data\n",
        "merged_df = merged_df.merge(festival_clean, on=['Year', 'Month'], how='left')\n",
        "\n",
        "# Merge temperature data\n",
        "merged_df = merged_df.merge(temperature_clean, on=['Year', 'Month'], how='left')\n",
        "\n",
        "# Merge IIP data\n",
        "merged_df = merged_df.merge(iip_clean, on=['Year', 'Month'], how='left')\n",
        "\n",
        "# Sort by date\n",
        "merged_df = merged_df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
        "print(f\"Date range: {merged_df['Date'].min()} to {merged_df['Date'].max()}\")\n",
        "print(f\"\\nMissing values:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Handle missing values\n",
        "if merged_df.isnull().sum().sum() > 0:\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    # Forward fill for small gaps\n",
        "    merged_df = merged_df.fillna(method='ffill').fillna(method='bfill')\n",
        "    print(\"Missing values handled\")\n",
        "\n",
        "# Save cleaned merged dataset\n",
        "merged_df.to_csv(f\"{FOLDERS['data_cleaned']}/merged_data.csv\", index=False)\n",
        "print(f\"\\nrged dataset saved to: {FOLDERS['data_cleaned']}/merged_data.csv\")\n",
        "\n",
        "\n",
        "print(\"FINAL DATASET PREVIEW\")\n",
        "\n",
        "print(merged_df.head(10))\n",
        "print(\"\\n\")\n",
        "print(merged_df.tail(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BY2Pa-dsFbdf",
        "outputId": "472c612e-42fa-459a-fdc0-d51057dc1f92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MERGING DATASETS\n",
            "\n",
            "Merged dataset shape: (126, 7)\n",
            "Date range: 2015-04-01 00:00:00 to 2025-08-01 00:00:00\n",
            "\n",
            "Missing values:\n",
            "Date                      0\n",
            "Year                      0\n",
            "Month                     0\n",
            "Requirement_MU            0\n",
            "Monthly_Festival_Index    0\n",
            "Temperature_Mean          1\n",
            "IIP_YoY                   1\n",
            "dtype: int64\n",
            "\n",
            "Handling missing values...\n",
            "Missing values handled\n",
            "\n",
            "rged dataset saved to: /content/drive/MyDrive/Time_Series_Project/data/cleaned/merged_data.csv\n",
            "FINAL DATASET PREVIEW\n",
            "        Date  Year  Month  Requirement_MU  Monthly_Festival_Index  Temperature_Mean  IIP_YoY\n",
            "0 2015-04-01  2015      4         89181.4                     8.5             27.74    0.041\n",
            "1 2015-05-01  2015      5         98315.0                     4.0             30.24    0.027\n",
            "2 2015-06-01  2015      6         92753.8                     1.5             29.15    0.038\n",
            "3 2015-07-01  2015      7         99105.2                     4.5             28.64    0.042\n",
            "4 2015-08-01  2015      8         99166.2                     6.5             28.05    0.064\n",
            "5 2015-09-01  2015      9         99019.7                    14.0             27.98    0.036\n",
            "6 2015-10-01  2015     10        100793.0                    16.5             27.03    0.098\n",
            "7 2015-11-01  2015     11         86681.0                     8.5             24.09   -0.032\n",
            "8 2015-12-01  2015     12         91066.8                     6.0             21.13   -0.013\n",
            "9 2016-01-01  2016      1         93090.0                     9.0             20.45   -0.015\n",
            "\n",
            "\n",
            "          Date  Year  Month  Requirement_MU  Monthly_Festival_Index  Temperature_Mean  IIP_YoY\n",
            "116 2024-12-01  2024     12        129582.0                     6.0             19.46    0.032\n",
            "117 2025-01-01  2025      1        136363.0                     9.0             19.02    0.050\n",
            "118 2025-02-01  2025      2        130604.0                     3.5             22.06    0.029\n",
            "119 2025-03-01  2025      3        146955.0                     9.0             25.52    0.030\n",
            "120 2025-04-01  2025      4        148066.0                    11.5             29.16    0.027\n",
            "121 2025-05-01  2025      5        147948.0                     4.0             29.57    0.012\n",
            "122 2025-06-01  2025      6        149183.0                     5.5             29.45    0.015\n",
            "123 2025-07-01  2025      7        153925.0                     2.5             27.93    0.048\n",
            "124 2025-07-01  2025      7        153925.0                     2.5             27.93    0.035\n",
            "125 2025-08-01  2025      8        149795.0                    13.5             27.51    0.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "ZuSJF37WG9db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(merged_df.describe())\n",
        "\n",
        "# Save statistics\n",
        "stats_df = merged_df.describe()\n",
        "stats_df.to_csv(f\"{FOLDERS['reports']}/descriptive_statistics.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "phWK-53ZMSEs",
        "outputId": "56798c88-cac2-48a1-b2f5-bcf0f1ccc2a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPLORATORY DATA ANALYSIS\n",
            "\n",
            "Descriptive Statistics:\n",
            "                                Date         Year       Month  Requirement_MU  Monthly_Festival_Index  Temperature_Mean     IIP_YoY\n",
            "count                            126   126.000000  126.000000      126.000000              126.000000        126.000000  126.000000\n",
            "mean   2020-06-15 14:05:42.857142784  2020.000000    6.484127   115398.657143                7.468254         26.069524    0.038373\n",
            "min              2015-04-01 00:00:00  2015.000000    1.000000    85030.000000                0.000000         19.020000   -0.347000\n",
            "25%              2017-11-08 12:00:00  2017.000000    4.000000   101285.350000                4.000000         23.472500    0.005500\n",
            "50%              2020-06-16 00:00:00  2020.000000    6.500000   110968.600000                6.750000         27.610000    0.031000\n",
            "75%              2023-01-24 06:00:00  2023.000000    9.000000   128646.500000               10.375000         28.510000    0.049750\n",
            "max              2025-08-01 00:00:00  2025.000000   12.000000   155346.000000               21.500000         31.080000    1.344000\n",
            "std                              NaN     3.059412    3.395842    18054.140070                4.645319          3.379905    0.133924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Visualization"
      ],
      "metadata": {
        "id": "KgBWLVtJMT_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[1/5] Creating time series visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "\n",
        "# Target variable\n",
        "axes[0].plot(merged_df['Date'], merged_df['Requirement_MU'], color='darkblue', linewidth=2)\n",
        "axes[0].set_title('Electricity Requirement (MU) - April 2015 to August 2025', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Requirement (MU)', fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Festival Index\n",
        "axes[1].plot(merged_df['Date'], merged_df['Monthly_Festival_Index'], color='darkorange', linewidth=2)\n",
        "axes[1].set_title('Monthly Festival Index', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Festival Index', fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Temperature\n",
        "axes[2].plot(merged_df['Date'], merged_df['Temperature_Mean'], color='darkred', linewidth=2)\n",
        "axes[2].set_title('Average Temperature (°C)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Temperature (°C)', fontsize=12)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# IIP YoY%\n",
        "axes[3].plot(merged_df['Date'], merged_df['IIP_YoY'], color='darkgreen', linewidth=2)\n",
        "axes[3].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "axes[3].set_title('IIP Year-over-Year Growth (%)', fontsize=14, fontweight='bold')\n",
        "axes[3].set_ylabel('IIP YoY %', fontsize=12)\n",
        "axes[3].set_xlabel('Date', fontsize=12)\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/01_time_series_overview.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 01_time_series_overview.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LUbRGysMG9wF",
        "outputId": "1a2618e2-fcbb-4fff-d711-e14830ee4e59"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/5] Creating time series visualizations...\n",
            "Saved: 01_time_series_overview.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation"
      ],
      "metadata": {
        "id": "ahZE7rPPHC-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[2/5] Creating correlation analysis...\")\n",
        "\n",
        "correlation_vars = ['Requirement_MU', 'Monthly_Festival_Index', 'Temperature_Mean', 'IIP_YoY']\n",
        "corr_matrix = merged_df[correlation_vars].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
        "            fmt='.3f', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix: Target and Exogenous Variables', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/02_correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 02_correlation_matrix.png\")\n",
        "\n",
        "# Save correlation matrix\n",
        "corr_matrix.to_csv(f\"{FOLDERS['reports']}/correlation_matrix.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nCT2yY-RHEcW",
        "outputId": "12dcc382-d016-4a03-d11f-8ef9d4dc530e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/5] Creating correlation analysis...\n",
            "Saved: 02_correlation_matrix.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution"
      ],
      "metadata": {
        "id": "oybIb4mgHHc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[3/5] Creating distribution plots...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Electricity Requirement\n",
        "axes[0, 0].hist(merged_df['Requirement_MU'], bins=30, color='darkblue', alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Distribution: Electricity Requirement', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Requirement (MU)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Festival Index\n",
        "axes[0, 1].hist(merged_df['Monthly_Festival_Index'], bins=20, color='darkorange', alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_title('Distribution: Festival Index', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Festival Index')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# Temperature\n",
        "axes[1, 0].hist(merged_df['Temperature_Mean'], bins=25, color='darkred', alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_title('Distribution: Temperature', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Temperature (°C)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# IIP YoY\n",
        "axes[1, 1].hist(merged_df['IIP_YoY'], bins=25, color='darkgreen', alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].set_title('Distribution: IIP YoY%', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('IIP YoY %')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/03_distributions.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 03_distributions.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f_L5BXRyHG9T",
        "outputId": "61bafaac-c4be-41c7-b133-88ea02759cd7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/5] Creating distribution plots...\n",
            "Saved: 03_distributions.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonal Decomposition"
      ],
      "metadata": {
        "id": "iK6Qu9G9HMah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[4/5] Performing seasonal decomposition...\")\n",
        "\n",
        "# Set Date as index for decomposition\n",
        "ts_data = merged_df.set_index('Date')['Requirement_MU']\n",
        "\n",
        "# Perform decomposition (additive)\n",
        "decomposition = seasonal_decompose(ts_data, model='additive', period=12)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "\n",
        "decomposition.observed.plot(ax=axes[0], color='darkblue', linewidth=2)\n",
        "axes[0].set_ylabel('Observed')\n",
        "axes[0].set_title('Seasonal Decomposition of Electricity Requirement', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "decomposition.trend.plot(ax=axes[1], color='darkorange', linewidth=2)\n",
        "axes[1].set_ylabel('Trend')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "decomposition.seasonal.plot(ax=axes[2], color='darkgreen', linewidth=2)\n",
        "axes[2].set_ylabel('Seasonal')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "decomposition.resid.plot(ax=axes[3], color='darkred', linewidth=1)\n",
        "axes[3].set_ylabel('Residual')\n",
        "axes[3].set_xlabel('Date')\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/04_seasonal_decomposition.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 04_seasonal_decomposition.png\")\n",
        "\n",
        "# Save decomposition components\n",
        "decomp_df = pd.DataFrame({\n",
        "    'Date': decomposition.observed.index,\n",
        "    'Observed': decomposition.observed.values,\n",
        "    'Trend': decomposition.trend.values,\n",
        "    'Seasonal': decomposition.seasonal.values,\n",
        "    'Residual': decomposition.resid.values\n",
        "})\n",
        "decomp_df.to_csv(f\"{FOLDERS['reports']}/seasonal_decomposition.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bdiF0WTsHODr",
        "outputId": "f5ddd6d8-74fe-45f6-92e6-2c08e5b8815b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/5] Performing seasonal decomposition...\n",
            "Saved: 04_seasonal_decomposition.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stationarity Tests"
      ],
      "metadata": {
        "id": "4JvaMn8eHaNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[5/5] Conducting stationarity tests...\")\n",
        "\n",
        "def adf_test(series, title=''):\n",
        "    \"\"\"Augmented Dickey-Fuller Test\"\"\"\n",
        "    result = adfuller(series.dropna(), autolag='AIC')\n",
        "    output = {\n",
        "        'Test Statistic': result[0],\n",
        "        'p-value': result[1],\n",
        "        'Lags Used': result[2],\n",
        "        'Observations': result[3],\n",
        "        'Critical Value (1%)': result[4]['1%'],\n",
        "        'Critical Value (5%)': result[4]['5%'],\n",
        "        'Critical Value (10%)': result[4]['10%']\n",
        "    }\n",
        "    return output\n",
        "\n",
        "def kpss_test(series, title=''):\n",
        "    \"\"\"KPSS Test\"\"\"\n",
        "    result = kpss(series.dropna(), regression='c', nlags='auto')\n",
        "    output = {\n",
        "        'Test Statistic': result[0],\n",
        "        'p-value': result[1],\n",
        "        'Lags Used': result[2],\n",
        "        'Critical Value (1%)': result[3]['1%'],\n",
        "        'Critical Value (2.5%)': result[3]['2.5%'],\n",
        "        'Critical Value (5%)': result[3]['5%'],\n",
        "        'Critical Value (10%)': result[3]['10%']\n",
        "    }\n",
        "    return output\n",
        "\n",
        "# Test original series\n",
        "adf_results = adf_test(merged_df['Requirement_MU'])\n",
        "kpss_results = kpss_test(merged_df['Requirement_MU'])\n",
        "\n",
        "print(\"\\n--- Augmented Dickey-Fuller Test (ADF) ---\")\n",
        "print(f\"ADF Statistic: {adf_results['Test Statistic']:.6f}\")\n",
        "print(f\"p-value: {adf_results['p-value']:.6f}\")\n",
        "print(f\"Critical Values:\")\n",
        "print(f\"  1%: {adf_results['Critical Value (1%)']:.3f}\")\n",
        "print(f\"  5%: {adf_results['Critical Value (5%)']:.3f}\")\n",
        "print(f\"  10%: {adf_results['Critical Value (10%)']:.3f}\")\n",
        "if adf_results['p-value'] < 0.05:\n",
        "    print(\"Series is STATIONARY (reject H0)\")\n",
        "else:\n",
        "    print(\"✗ Series is NON-STATIONARY (fail to reject H0)\")\n",
        "\n",
        "print(\"\\n--- KPSS Test ---\")\n",
        "print(f\"KPSS Statistic: {kpss_results['Test Statistic']:.6f}\")\n",
        "print(f\"p-value: {kpss_results['p-value']:.6f}\")\n",
        "print(f\"Critical Values:\")\n",
        "print(f\"  1%: {kpss_results['Critical Value (1%)']:.3f}\")\n",
        "print(f\"  5%: {kpss_results['Critical Value (5%)']:.3f}\")\n",
        "print(f\"  10%: {kpss_results['Critical Value (10%)']:.3f}\")\n",
        "if kpss_results['p-value'] < 0.05:\n",
        "    print(\"✗ Series is NON-STATIONARY (reject H0)\")\n",
        "else:\n",
        "    print(\"Series is STATIONARY (fail to reject H0)\")\n",
        "\n",
        "# Save stationarity test results\n",
        "stationarity_results = pd.DataFrame({\n",
        "    'Test': ['ADF', 'KPSS'],\n",
        "    'Test_Statistic': [adf_results['Test Statistic'], kpss_results['Test Statistic']],\n",
        "    'p_value': [adf_results['p-value'], kpss_results['p-value']],\n",
        "    'Stationary': [adf_results['p-value'] < 0.05, kpss_results['p-value'] >= 0.05]\n",
        "})\n",
        "stationarity_results.to_csv(f\"{FOLDERS['reports']}/stationarity_tests.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2--1PzePHapk",
        "outputId": "da87e932-4e44-457a-dd31-ccc5eb8a5b8a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5/5] Conducting stationarity tests...\n",
            "\n",
            "--- Augmented Dickey-Fuller Test (ADF) ---\n",
            "ADF Statistic: 0.766288\n",
            "p-value: 0.991072\n",
            "Critical Values:\n",
            "  1%: -3.490\n",
            "  5%: -2.887\n",
            "  10%: -2.581\n",
            "✗ Series is NON-STATIONARY (fail to reject H0)\n",
            "\n",
            "--- KPSS Test ---\n",
            "KPSS Statistic: 1.682922\n",
            "p-value: 0.010000\n",
            "Critical Values:\n",
            "  1%: 0.739\n",
            "  5%: 0.463\n",
            "  10%: 0.347\n",
            "✗ Series is NON-STATIONARY (reject H0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1284572771.py:19: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
            "look-up table. The actual p-value is smaller than the p-value returned.\n",
            "\n",
            "  result = kpss(series.dropna(), regression='c', nlags='auto')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN-VALIDATION-TEST SPLIT"
      ],
      "metadata": {
        "id": "PVsE0HNwKThX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"TRAIN-VALIDATION-TEST SPLIT\")\n",
        "\n",
        "\n",
        "# Total observations: 125 months (April 2015 - August 2025)\n",
        "# Train: 70% = 87 months\n",
        "# Validation: 15% = 19 months\n",
        "# Test: 15% = 19 months\n",
        "\n",
        "total_obs = len(merged_df)\n",
        "train_size = int(total_obs * 0.70)\n",
        "val_size = int(total_obs * 0.15)\n",
        "test_size = total_obs - train_size - val_size\n",
        "\n",
        "# Create splits\n",
        "train_data = merged_df.iloc[:train_size].copy()\n",
        "val_data = merged_df.iloc[train_size:train_size+val_size].copy()\n",
        "test_data = merged_df.iloc[train_size+val_size:].copy()\n",
        "\n",
        "print(f\"\\nTotal observations: {total_obs}\")\n",
        "print(f\"\\nTrain Set: {len(train_data)} months ({len(train_data)/total_obs*100:.1f}%)\")\n",
        "print(f\"  Date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "print(f\"\\nValidation Set: {len(val_data)} months ({len(val_data)/total_obs*100:.1f}%)\")\n",
        "print(f\"  Date range: {val_data['Date'].min()} to {val_data['Date'].max()}\")\n",
        "print(f\"\\nTest Set: {len(test_data)} months ({len(test_data)/total_obs*100:.1f}%)\")\n",
        "print(f\"  Date range: {test_data['Date'].min()} to {test_data['Date'].max()}\")\n",
        "\n",
        "# Save split information\n",
        "split_info = pd.DataFrame({\n",
        "    'Split': ['Train', 'Validation', 'Test'],\n",
        "    'Start_Date': [train_data['Date'].min(), val_data['Date'].min(), test_data['Date'].min()],\n",
        "    'End_Date': [train_data['Date'].max(), val_data['Date'].max(), test_data['Date'].max()],\n",
        "    'N_Observations': [len(train_data), len(val_data), len(test_data)],\n",
        "    'Percentage': [len(train_data)/total_obs*100, len(val_data)/total_obs*100, len(test_data)/total_obs*100]\n",
        "})\n",
        "split_info.to_csv(f\"{FOLDERS['reports']}/data_split_info.csv\", index=False)\n",
        "\n",
        "# Visualize the split\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(train_data['Date'], train_data['Requirement_MU'], label='Train', color='blue', linewidth=2)\n",
        "plt.plot(val_data['Date'], val_data['Requirement_MU'], label='Validation', color='orange', linewidth=2)\n",
        "plt.plot(test_data['Date'], test_data['Requirement_MU'], label='Test', color='green', linewidth=2)\n",
        "plt.axvline(x=train_data['Date'].max(), color='red', linestyle='--', alpha=0.5, label='Train-Val Split')\n",
        "plt.axvline(x=val_data['Date'].max(), color='red', linestyle='--', alpha=0.5, label='Val-Test Split')\n",
        "plt.title('Train-Validation-Test Split', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Electricity Requirement (MU)', fontsize=12)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/05_train_val_test_split.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"\\nSaved: 05_train_val_test_split.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n-NvElQZKUWW",
        "outputId": "6320936d-1ead-4fe8-a79e-b410bab3b12c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN-VALIDATION-TEST SPLIT\n",
            "\n",
            "Total observations: 126\n",
            "\n",
            "Train Set: 88 months (69.8%)\n",
            "  Date range: 2015-04-01 00:00:00 to 2022-07-01 00:00:00\n",
            "\n",
            "Validation Set: 18 months (14.3%)\n",
            "  Date range: 2022-08-01 00:00:00 to 2024-01-01 00:00:00\n",
            "\n",
            "Test Set: 20 months (15.9%)\n",
            "  Date range: 2024-02-01 00:00:00 to 2025-08-01 00:00:00\n",
            "\n",
            "Saved: 05_train_val_test_split.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACF AND PACF PLOTS"
      ],
      "metadata": {
        "id": "O5Pj-KFnKZ7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ACF AND PACF ANALYSIS\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "# ACF\n",
        "plot_acf(train_data['Requirement_MU'], lags=40, ax=axes[0])\n",
        "axes[0].set_title('Autocorrelation Function (ACF)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Lag', fontsize=12)\n",
        "axes[0].set_ylabel('ACF', fontsize=12)\n",
        "\n",
        "# PACF\n",
        "plot_pacf(train_data['Requirement_MU'], lags=40, ax=axes[1], method='ywm')\n",
        "axes[1].set_title('Partial Autocorrelation Function (PACF)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Lag', fontsize=12)\n",
        "axes[1].set_ylabel('PACF', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/06_acf_pacf_plots.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 06_acf_pacf_plots.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cetm_bVWKb1N",
        "outputId": "665d87e2-0582-40fe-83ef-8659182d6ae6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACF AND PACF ANALYSIS\n",
            "Saved: 06_acf_pacf_plots.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EVALUATION METRICS FUNCTIONS"
      ],
      "metadata": {
        "id": "JqKrHVDOKfRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"DEFINING EVALUATION FUNCTIONS\")\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, model_name=''):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Directional Accuracy\n",
        "    direction_true = np.diff(y_true) > 0\n",
        "    direction_pred = np.diff(y_pred) > 0\n",
        "    directional_accuracy = np.mean(direction_true == direction_pred) * 100\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2_Score': r2,\n",
        "        'Directional_Accuracy': directional_accuracy\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def print_metrics(metrics):\n",
        "    \"\"\"Pretty print metrics\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {metrics['Model']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"MAE (Mean Absolute Error):         {metrics['MAE']:,.2f} MU\")\n",
        "    print(f\"RMSE (Root Mean Squared Error):    {metrics['RMSE']:,.2f} MU\")\n",
        "    print(f\"MAPE (Mean Absolute % Error):      {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"R² Score:                          {metrics['R2_Score']:.4f}\")\n",
        "    print(f\"Directional Accuracy:              {metrics['Directional_Accuracy']:.2f}%\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "def plot_residuals(residuals, model_name, save_path):\n",
        "    \"\"\"Plot residual diagnostics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Residuals over time\n",
        "    axes[0, 0].plot(residuals, color='darkblue', linewidth=1)\n",
        "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
        "    axes[0, 0].set_title(f'{model_name}: Residuals Over Time', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Observation')\n",
        "    axes[0, 0].set_ylabel('Residuals')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Histogram\n",
        "    axes[0, 1].hist(residuals, bins=30, color='darkblue', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 1].set_title(f'{model_name}: Residuals Distribution', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Residuals')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Q-Q plot\n",
        "    from scipy import stats\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
        "    axes[1, 0].set_title(f'{model_name}: Q-Q Plot', fontweight='bold')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # ACF of residuals\n",
        "    plot_acf(residuals, lags=min(40, len(residuals)//2), ax=axes[1, 1])\n",
        "    axes[1, 1].set_title(f'{model_name}: ACF of Residuals', fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "print(\"Evaluation functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BXDbFqsBKh6F",
        "outputId": "e4cd60cc-4c1c-4ddb-c82a-57f2ea0fdd8a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEFINING EVALUATION FUNCTIONS\n",
            "Evaluation functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL 1 - ARIMA with Grid Search"
      ],
      "metadata": {
        "id": "7vSDN2viKm5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL 1: ARIMA WITH GRID SEARCH\")\n",
        "\n",
        "# Define parameter ranges for grid search\n",
        "p_values = range(0, 4)\n",
        "d_values = range(0, 3)\n",
        "q_values = range(0, 4)\n",
        "\n",
        "# Grid search for ARIMA\n",
        "best_aic = np.inf\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "arima_results_list = []\n",
        "\n",
        "total_combinations = len(p_values) * len(d_values) * len(q_values)\n",
        "current_combination = 0\n",
        "\n",
        "print(f\"\\nTesting {total_combinations} parameter combinations...\")\n",
        "\n",
        "for p in p_values:\n",
        "    for d in d_values:\n",
        "        for q in q_values:\n",
        "            current_combination += 1\n",
        "            try:\n",
        "                model = ARIMA(train_data['Requirement_MU'], order=(p, d, q))\n",
        "                fitted_model = model.fit()\n",
        "\n",
        "                # Validation predictions\n",
        "                val_pred = fitted_model.forecast(steps=len(val_data))\n",
        "                val_mae = mean_absolute_error(val_data['Requirement_MU'], val_pred)\n",
        "\n",
        "                arima_results_list.append({\n",
        "                    'p': p, 'd': d, 'q': q,\n",
        "                    'AIC': fitted_model.aic,\n",
        "                    'BIC': fitted_model.bic,\n",
        "                    'Val_MAE': val_mae\n",
        "                })\n",
        "\n",
        "                # Track best model by AIC\n",
        "                if fitted_model.aic < best_aic:\n",
        "                    best_aic = fitted_model.aic\n",
        "                    best_params = (p, d, q)\n",
        "                    best_model = fitted_model\n",
        "\n",
        "                if current_combination % 10 == 0:\n",
        "                    print(f\"  Progress: {current_combination}/{total_combinations} combinations tested...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "print(f\"\\nGrid search completed!\")\n",
        "print(f\"\\nBest ARIMA parameters: {best_params}\")\n",
        "print(f\"Best AIC: {best_aic:.2f}\")\n",
        "\n",
        "# Save grid search results\n",
        "arima_grid_df = pd.DataFrame(arima_results_list)\n",
        "arima_grid_df = arima_grid_df.sort_values('AIC')\n",
        "arima_grid_df.to_csv(f\"{FOLDERS['reports']}/ARIMA_grid_search_results.csv\", index=False)\n",
        "\n",
        "# Display top 10 models\n",
        "print(\"\\nTop 10 ARIMA models by AIC:\")\n",
        "print(arima_grid_df.head(10))\n",
        "\n",
        "# Final ARIMA model with best parameters\n",
        "print(f\"\\nFitting final ARIMA{best_params} model on full training data...\")\n",
        "final_arima = ARIMA(train_data['Requirement_MU'], order=best_params)\n",
        "fitted_arima = final_arima.fit()\n",
        "\n",
        "print(\"\\n\" + str(fitted_arima.summary()))\n",
        "\n",
        "# Predictions\n",
        "arima_train_pred = fitted_arima.fittedvalues\n",
        "arima_val_pred = fitted_arima.forecast(steps=len(val_data))\n",
        "arima_test_pred = fitted_arima.forecast(steps=len(val_data) + len(test_data))[len(val_data):]\n",
        "\n",
        "# Metrics on validation set\n",
        "arima_val_metrics = calculate_metrics(\n",
        "    val_data['Requirement_MU'].values,\n",
        "    arima_val_pred.values,\n",
        "    model_name='ARIMA_Validation'\n",
        ")\n",
        "print_metrics(arima_val_metrics)\n",
        "\n",
        "# Metrics on test set\n",
        "arima_test_metrics = calculate_metrics(\n",
        "    test_data['Requirement_MU'].values,\n",
        "    arima_test_pred.values,\n",
        "    model_name='ARIMA_Test'\n",
        ")\n",
        "print_metrics(arima_test_metrics)\n",
        "\n",
        "# Residual analysis\n",
        "arima_residuals = fitted_arima.resid\n",
        "plot_residuals(arima_residuals, 'ARIMA', f\"{FOLDERS['results_viz']}/07_ARIMA_residuals.png\")\n",
        "print(\"\\nSaved: 07_ARIMA_residuals.png\")\n",
        "\n",
        "# Save model summary\n",
        "with open(f\"{FOLDERS['reports']}/ARIMA_model_summary.txt\", 'w') as f:\n",
        "    f.write(str(fitted_arima.summary()))\n",
        "\n",
        "print(\"\\nARIMA model completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UoVdgZKcKmoF",
        "outputId": "6936bad7-578d-4958-d057-a8bbcbf9a45e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 1: ARIMA WITH GRID SEARCH\n",
            "\n",
            "Testing 48 parameter combinations...\n",
            "  Progress: 10/48 combinations tested...\n",
            "  Progress: 20/48 combinations tested...\n",
            "  Progress: 30/48 combinations tested...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Progress: 40/48 combinations tested...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grid search completed!\n",
            "\n",
            "Best ARIMA parameters: (0, 1, 0)\n",
            "Best AIC: 1795.12\n",
            "\n",
            "Top 10 ARIMA models by AIC:\n",
            "    p  d  q          AIC          BIC      Val_MAE\n",
            "4   0  1  0  1795.119707  1797.585615  8761.111111\n",
            "5   0  1  1  1797.066820  1801.998636  8761.111111\n",
            "16  1  1  0  1797.181271  1802.113088  8760.969133\n",
            "17  1  1  1  1798.369825  1805.767549  8824.418553\n",
            "6   0  1  2  1798.749459  1806.147184  8773.718551\n",
            "28  2  1  0  1798.806342  1806.204066  8772.221498\n",
            "43  3  1  3  1799.579296  1816.840652  8510.014152\n",
            "18  1  1  2  1799.609476  1809.473109  8767.174855\n",
            "29  2  1  1  1800.303273  1810.166906  8806.149432\n",
            "7   0  1  3  1800.375364  1810.238997  8768.295257\n",
            "\n",
            "Fitting final ARIMA(0, 1, 0) model on full training data...\n",
            "\n",
            "                               SARIMAX Results                                \n",
            "==============================================================================\n",
            "Dep. Variable:         Requirement_MU   No. Observations:                   88\n",
            "Model:                 ARIMA(0, 1, 0)   Log Likelihood                -896.560\n",
            "Date:                Thu, 20 Nov 2025   AIC                           1795.120\n",
            "Time:                        18:12:54   BIC                           1797.586\n",
            "Sample:                             0   HQIC                          1796.113\n",
            "                                 - 88                                         \n",
            "Covariance Type:                  opg                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "sigma2      5.171e+07   6.85e+06      7.545      0.000    3.83e+07    6.51e+07\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   5.11   Jarque-Bera (JB):                 1.53\n",
            "Prob(Q):                              0.02   Prob(JB):                         0.47\n",
            "Heteroskedasticity (H):               3.20   Skew:                             0.20\n",
            "Prob(H) (two-sided):                  0.00   Kurtosis:                         3.51\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
            "\n",
            "============================================================\n",
            "Model: ARIMA_Validation\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         8,761.11 MU\n",
            "RMSE (Root Mean Squared Error):    10,659.01 MU\n",
            "MAPE (Mean Absolute % Error):      6.74%\n",
            "R² Score:                          -0.0134\n",
            "Directional Accuracy:              41.18%\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Model: ARIMA_Test\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         15,055.40 MU\n",
            "RMSE (Root Mean Squared Error):    17,174.75 MU\n",
            "MAPE (Mean Absolute % Error):      10.18%\n",
            "R² Score:                          -2.4903\n",
            "Directional Accuracy:              52.63%\n",
            "============================================================\n",
            "\n",
            "Saved: 07_ARIMA_residuals.png\n",
            "\n",
            "ARIMA model completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL 2 - SARIMA with Grid Search"
      ],
      "metadata": {
        "id": "cdz6DgifKtW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL 2: SARIMA WITH GRID SEARCH\")\n",
        "\n",
        "# Define parameter ranges\n",
        "p_values = range(0, 3)\n",
        "d_values = range(0, 2)\n",
        "q_values = range(0, 3)\n",
        "P_values = range(0, 2)\n",
        "D_values = range(0, 2)\n",
        "Q_values = range(0, 2)\n",
        "m = 12  # Monthly seasonality\n",
        "\n",
        "best_aic_sarima = np.inf\n",
        "best_params_sarima = None\n",
        "best_model_sarima = None\n",
        "\n",
        "sarima_results_list = []\n",
        "\n",
        "total_combinations = len(p_values) * len(d_values) * len(q_values) * len(P_values) * len(D_values) * len(Q_values)\n",
        "current_combination = 0\n",
        "\n",
        "print(f\"\\nTesting {total_combinations} parameter combinations...\")\n",
        "\n",
        "for p in p_values:\n",
        "    for d in d_values:\n",
        "        for q in q_values:\n",
        "            for P in P_values:\n",
        "                for D in D_values:\n",
        "                    for Q in Q_values:\n",
        "                        current_combination += 1\n",
        "                        try:\n",
        "                            model = SARIMAX(\n",
        "                                train_data['Requirement_MU'],\n",
        "                                order=(p, d, q),\n",
        "                                seasonal_order=(P, D, Q, m)\n",
        "                            )\n",
        "                            fitted_model = model.fit(disp=False, maxiter=200)\n",
        "\n",
        "                            # Validation predictions\n",
        "                            val_pred = fitted_model.forecast(steps=len(val_data))\n",
        "                            val_mae = mean_absolute_error(val_data['Requirement_MU'], val_pred)\n",
        "\n",
        "                            sarima_results_list.append({\n",
        "                                'p': p, 'd': d, 'q': q,\n",
        "                                'P': P, 'D': D, 'Q': Q, 'm': m,\n",
        "                                'AIC': fitted_model.aic,\n",
        "                                'BIC': fitted_model.bic,\n",
        "                                'Val_MAE': val_mae\n",
        "                            })\n",
        "\n",
        "                            if fitted_model.aic < best_aic_sarima:\n",
        "                                best_aic_sarima = fitted_model.aic\n",
        "                                best_params_sarima = ((p, d, q), (P, D, Q, m))\n",
        "                                best_model_sarima = fitted_model\n",
        "\n",
        "                            if current_combination % 20 == 0:\n",
        "                                print(f\"  Progress: {current_combination}/{total_combinations} combinations tested...\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "print(f\"\\nGrid search completed!\")\n",
        "print(f\"\\nBest SARIMA parameters:\")\n",
        "print(f\"  Order (p,d,q): {best_params_sarima[0]}\")\n",
        "print(f\"  Seasonal (P,D,Q,m): {best_params_sarima[1]}\")\n",
        "print(f\"  Best AIC: {best_aic_sarima:.2f}\")\n",
        "\n",
        "# Save grid search results\n",
        "sarima_grid_df = pd.DataFrame(sarima_results_list)\n",
        "sarima_grid_df = sarima_grid_df.sort_values('AIC')\n",
        "sarima_grid_df.to_csv(f\"{FOLDERS['reports']}/SARIMA_grid_search_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop 10 SARIMA models by AIC:\")\n",
        "print(sarima_grid_df.head(10))\n",
        "\n",
        "# Final SARIMA model\n",
        "print(f\"\\nFitting final SARIMA model on training data...\")\n",
        "final_sarima = SARIMAX(\n",
        "    train_data['Requirement_MU'],\n",
        "    order=best_params_sarima[0],\n",
        "    seasonal_order=best_params_sarima[1]\n",
        ")\n",
        "fitted_sarima = final_sarima.fit(disp=False)\n",
        "\n",
        "print(\"\\n\" + str(fitted_sarima.summary()))\n",
        "\n",
        "# Predictions\n",
        "sarima_train_pred = fitted_sarima.fittedvalues\n",
        "sarima_val_pred = fitted_sarima.forecast(steps=len(val_data))\n",
        "sarima_test_pred = fitted_sarima.forecast(steps=len(val_data) + len(test_data))[len(val_data):]\n",
        "\n",
        "# Metrics on validation set\n",
        "sarima_val_metrics = calculate_metrics(\n",
        "    val_data['Requirement_MU'].values,\n",
        "    sarima_val_pred.values,\n",
        "    model_name='SARIMA_Validation'\n",
        ")\n",
        "print_metrics(sarima_val_metrics)\n",
        "\n",
        "# Metrics on test set\n",
        "sarima_test_metrics = calculate_metrics(\n",
        "    test_data['Requirement_MU'].values,\n",
        "    sarima_test_pred.values,\n",
        "    model_name='SARIMA_Test'\n",
        ")\n",
        "print_metrics(sarima_test_metrics)\n",
        "\n",
        "# Residual analysis\n",
        "sarima_residuals = fitted_sarima.resid\n",
        "plot_residuals(sarima_residuals, 'SARIMA', f\"{FOLDERS['results_viz']}/08_SARIMA_residuals.png\")\n",
        "print(\"\\nSaved: 08_SARIMA_residuals.png\")\n",
        "\n",
        "# Save model summary\n",
        "with open(f\"{FOLDERS['reports']}/SARIMA_model_summary.txt\", 'w') as f:\n",
        "    f.write(str(fitted_sarima.summary()))\n",
        "\n",
        "print(\"\\nSARIMA model completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ePtthy2MKv_s",
        "outputId": "13621c50-5eae-47df-ea47-195dde11aa2e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 2: SARIMA WITH GRID SEARCH\n",
            "\n",
            "Testing 144 parameter combinations...\n",
            "  Progress: 20/144 combinations tested...\n",
            "  Progress: 40/144 combinations tested...\n",
            "  Progress: 60/144 combinations tested...\n",
            "  Progress: 80/144 combinations tested...\n",
            "  Progress: 100/144 combinations tested...\n",
            "  Progress: 120/144 combinations tested...\n",
            "  Progress: 140/144 combinations tested...\n",
            "\n",
            "Grid search completed!\n",
            "\n",
            "Best SARIMA parameters:\n",
            "  Order (p,d,q): (0, 1, 2)\n",
            "  Seasonal (P,D,Q,m): (1, 1, 1, 12)\n",
            "  Best AIC: 1547.44\n",
            "\n",
            "Top 10 SARIMA models by AIC:\n",
            "     p  d  q  P  D  Q   m          AIC          BIC      Val_MAE\n",
            "47   0  1  2  1  1  1  12  1547.435618  1559.023058  4949.424004\n",
            "79   1  1  0  1  1  1  12  1547.652855  1556.922807  5395.141266\n",
            "39   0  1  1  1  1  1  12  1547.783361  1557.053314  5353.613875\n",
            "127  2  1  0  1  1  1  12  1547.868247  1559.455687  4984.676075\n",
            "87   1  1  1  1  1  1  12  1548.449688  1560.037129  4964.731541\n",
            "95   1  1  2  1  1  1  12  1549.146778  1563.051706  4940.174486\n",
            "135  2  1  1  1  1  1  12  1549.425095  1563.330024  4933.308105\n",
            "75   1  1  0  0  1  1  12  1549.902577  1556.855041  6762.238072\n",
            "35   0  1  1  0  1  1  12  1549.975320  1556.927785  6709.600667\n",
            "30   0  1  0  1  1  0  12  1550.226324  1554.861300  7484.189187\n",
            "\n",
            "Fitting final SARIMA model on training data...\n",
            "\n",
            "                                      SARIMAX Results                                       \n",
            "============================================================================================\n",
            "Dep. Variable:                       Requirement_MU   No. Observations:                   88\n",
            "Model:             SARIMAX(0, 1, 2)x(1, 1, [1], 12)   Log Likelihood                -768.718\n",
            "Date:                              Thu, 20 Nov 2025   AIC                           1547.436\n",
            "Time:                                      18:13:54   BIC                           1559.023\n",
            "Sample:                                           0   HQIC                          1552.062\n",
            "                                               - 88                                         \n",
            "Covariance Type:                                opg                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "ma.L1         -0.0354      0.081     -0.437      0.662      -0.194       0.123\n",
            "ma.L2         -0.0973      0.105     -0.929      0.353      -0.303       0.108\n",
            "ar.S.L12       0.4925      0.175      2.813      0.005       0.149       0.836\n",
            "ma.S.L12      -0.8554      0.250     -3.423      0.001      -1.345      -0.366\n",
            "sigma2      5.172e+07   4.09e-09   1.26e+16      0.000    5.17e+07    5.17e+07\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   0.85   Jarque-Bera (JB):                15.57\n",
            "Prob(Q):                              0.36   Prob(JB):                         0.00\n",
            "Heteroskedasticity (H):               2.81   Skew:                            -0.87\n",
            "Prob(H) (two-sided):                  0.01   Kurtosis:                         4.40\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
            "[2] Covariance matrix is singular or near-singular, with condition number 3.27e+31. Standard errors may be unstable.\n",
            "\n",
            "============================================================\n",
            "Model: SARIMA_Validation\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         4,949.42 MU\n",
            "RMSE (Root Mean Squared Error):    5,869.98 MU\n",
            "MAPE (Mean Absolute % Error):      3.78%\n",
            "R² Score:                          0.6927\n",
            "Directional Accuracy:              82.35%\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Model: SARIMA_Test\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         6,215.46 MU\n",
            "RMSE (Root Mean Squared Error):    7,024.21 MU\n",
            "MAPE (Mean Absolute % Error):      4.37%\n",
            "R² Score:                          0.4162\n",
            "Directional Accuracy:              73.68%\n",
            "============================================================\n",
            "\n",
            "Saved: 08_SARIMA_residuals.png\n",
            "\n",
            "SARIMA model completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL 3 - SARIMAX with Grid Search"
      ],
      "metadata": {
        "id": "t2je03Z5K0jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"MODEL 3: SARIMAX WITH EXOGENOUS VARIABLES AND GRID SEARCH\")\n",
        "\n",
        "\n",
        "exog_vars = ['Monthly_Festival_Index', 'Temperature_Mean', 'IIP_YoY']\n",
        "train_exog = train_data[exog_vars]\n",
        "val_exog = val_data[exog_vars]\n",
        "test_exog = test_data[exog_vars]\n",
        "\n",
        "print(f\"Exogenous variables: {exog_vars}\")\n",
        "print(f\"Train exog shape: {train_exog.shape}\")\n",
        "print(f\"Val exog shape: {val_exog.shape}\")\n",
        "print(f\"Test exog shape: {test_exog.shape}\")\n",
        "\n",
        "# Use smaller grid due to computational cost with exogenous variables\n",
        "p_values = range(0, 3)\n",
        "d_values = range(0, 2)\n",
        "q_values = range(0, 3)\n",
        "P_values = range(0, 2)\n",
        "D_values = range(0, 2)\n",
        "Q_values = range(0, 2)\n",
        "m = 12\n",
        "\n",
        "best_aic_sarimax = np.inf\n",
        "best_params_sarimax = None\n",
        "best_model_sarimax = None\n",
        "\n",
        "sarimax_results_list = []\n",
        "\n",
        "total_combinations = len(p_values) * len(d_values) * len(q_values) * len(P_values) * len(D_values) * len(Q_values)\n",
        "current_combination = 0\n",
        "\n",
        "print(f\"\\nTesting {total_combinations} parameter combinations with exogenous variables...\")\n",
        "\n",
        "for p in p_values:\n",
        "    for d in d_values:\n",
        "        for q in q_values:\n",
        "            for P in P_values:\n",
        "                for D in D_values:\n",
        "                    for Q in Q_values:\n",
        "                        current_combination += 1\n",
        "                        try:\n",
        "                            model = SARIMAX(\n",
        "                                train_data['Requirement_MU'],\n",
        "                                exog=train_exog,\n",
        "                                order=(p, d, q),\n",
        "                                seasonal_order=(P, D, Q, m)\n",
        "                            )\n",
        "                            fitted_model = model.fit(disp=False, maxiter=200)\n",
        "\n",
        "                            # Validation predictions\n",
        "                            val_pred = fitted_model.forecast(steps=len(val_data), exog=val_exog)\n",
        "                            val_mae = mean_absolute_error(val_data['Requirement_MU'], val_pred)\n",
        "\n",
        "                            sarimax_results_list.append({\n",
        "                                'p': p, 'd': d, 'q': q,\n",
        "                                'P': P, 'D': D, 'Q': Q, 'm': m,\n",
        "                                'AIC': fitted_model.aic,\n",
        "                                'BIC': fitted_model.bic,\n",
        "                                'Val_MAE': val_mae\n",
        "                            })\n",
        "\n",
        "                            if fitted_model.aic < best_aic_sarimax:\n",
        "                                best_aic_sarimax = fitted_model.aic\n",
        "                                best_params_sarimax = ((p, d, q), (P, D, Q, m))\n",
        "                                best_model_sarimax = fitted_model\n",
        "\n",
        "                            if current_combination % 20 == 0:\n",
        "                                print(f\"  Progress: {current_combination}/{total_combinations} combinations tested...\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "print(f\"\\nGrid search completed!\")\n",
        "print(f\"\\nBest SARIMAX parameters:\")\n",
        "print(f\"  Order (p,d,q): {best_params_sarimax[0]}\")\n",
        "print(f\"  Seasonal (P,D,Q,m): {best_params_sarimax[1]}\")\n",
        "print(f\"  Best AIC: {best_aic_sarimax:.2f}\")\n",
        "\n",
        "# Save grid search results\n",
        "sarimax_grid_df = pd.DataFrame(sarimax_results_list)\n",
        "sarimax_grid_df = sarimax_grid_df.sort_values('AIC')\n",
        "sarimax_grid_df.to_csv(f\"{FOLDERS['reports']}/SARIMAX_grid_search_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop 10 SARIMAX models by AIC:\")\n",
        "print(sarimax_grid_df.head(10))\n",
        "\n",
        "# Final SARIMAX model\n",
        "print(f\"\\nFitting final SARIMAX model on training data...\")\n",
        "final_sarimax = SARIMAX(\n",
        "    train_data['Requirement_MU'],\n",
        "    exog=train_exog,\n",
        "    order=best_params_sarimax[0],\n",
        "    seasonal_order=best_params_sarimax[1]\n",
        ")\n",
        "fitted_sarimax = final_sarimax.fit(disp=False)\n",
        "\n",
        "print(\"\\n\" + str(fitted_sarimax.summary()))\n",
        "\n",
        "# Predictions\n",
        "sarimax_train_pred = fitted_sarimax.fittedvalues\n",
        "sarimax_val_pred = fitted_sarimax.forecast(steps=len(val_data), exog=val_exog)\n",
        "sarimax_test_pred = fitted_sarimax.forecast(steps=len(test_data), exog=test_exog)\n",
        "\n",
        "# Metrics on validation set\n",
        "sarimax_val_metrics = calculate_metrics(\n",
        "    val_data['Requirement_MU'].values,\n",
        "    sarimax_val_pred.values,\n",
        "    model_name='SARIMAX_Validation'\n",
        ")\n",
        "print_metrics(sarimax_val_metrics)\n",
        "\n",
        "# Metrics on test set\n",
        "sarimax_test_metrics = calculate_metrics(\n",
        "    test_data['Requirement_MU'].values,\n",
        "    sarimax_test_pred.values,\n",
        "    model_name='SARIMAX_Test'\n",
        ")\n",
        "print_metrics(sarimax_test_metrics)\n",
        "\n",
        "# Residual analysis\n",
        "sarimax_residuals = fitted_sarimax.resid\n",
        "plot_residuals(sarimax_residuals, 'SARIMAX', f\"{FOLDERS['results_viz']}/09_SARIMAX_residuals.png\")\n",
        "print(\"\\nSaved: 09_SARIMAX_residuals.png\")\n",
        "\n",
        "# Save model summary\n",
        "with open(f\"{FOLDERS['reports']}/SARIMAX_model_summary.txt\", 'w') as f:\n",
        "    f.write(str(fitted_sarimax.summary()))\n",
        "\n",
        "print(\"\\nSARIMAX model completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1VX2C9j7K24G",
        "outputId": "1de0f73a-cd4a-4115-8329-2728b54958ba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 3: SARIMAX WITH EXOGENOUS VARIABLES AND GRID SEARCH\n",
            "Exogenous variables: ['Monthly_Festival_Index', 'Temperature_Mean', 'IIP_YoY']\n",
            "Train exog shape: (88, 3)\n",
            "Val exog shape: (18, 3)\n",
            "Test exog shape: (20, 3)\n",
            "\n",
            "Testing 144 parameter combinations with exogenous variables...\n",
            "  Progress: 20/144 combinations tested...\n",
            "  Progress: 40/144 combinations tested...\n",
            "  Progress: 60/144 combinations tested...\n",
            "  Progress: 80/144 combinations tested...\n",
            "  Progress: 100/144 combinations tested...\n",
            "  Progress: 120/144 combinations tested...\n",
            "  Progress: 140/144 combinations tested...\n",
            "\n",
            "Grid search completed!\n",
            "\n",
            "Best SARIMAX parameters:\n",
            "  Order (p,d,q): (1, 1, 1)\n",
            "  Seasonal (P,D,Q,m): (0, 1, 1, 12)\n",
            "  Best AIC: 1513.48\n",
            "\n",
            "Top 10 SARIMAX models by AIC:\n",
            "     p  d  q  P  D  Q   m          AIC          BIC      Val_MAE\n",
            "83   1  1  1  0  1  1  12  1513.483400  1529.705817  4813.691268\n",
            "43   0  1  2  0  1  1  12  1515.084232  1531.306649  4497.923924\n",
            "139  2  1  2  0  1  1  12  1515.994597  1536.851990  7848.861942\n",
            "143  2  1  2  1  1  1  12  1517.946529  1541.121410  4773.569485\n",
            "131  2  1  1  0  1  1  12  1518.415205  1536.955110  4422.254332\n",
            "39   0  1  1  1  1  1  12  1518.823619  1535.046035  4483.298662\n",
            "27   0  1  0  0  1  1  12  1519.242197  1530.829638  5006.891321\n",
            "87   1  1  1  1  1  1  12  1519.342552  1537.882457  4280.535670\n",
            "35   0  1  1  0  1  1  12  1520.133609  1534.038537  4429.469552\n",
            "135  2  1  1  1  1  1  12  1520.143015  1541.000408  4268.918724\n",
            "\n",
            "Fitting final SARIMAX model on training data...\n",
            "\n",
            "                                     SARIMAX Results                                      \n",
            "==========================================================================================\n",
            "Dep. Variable:                     Requirement_MU   No. Observations:                   88\n",
            "Model:             SARIMAX(1, 1, 1)x(0, 1, 1, 12)   Log Likelihood                -749.742\n",
            "Date:                            Thu, 20 Nov 2025   AIC                           1513.483\n",
            "Time:                                    18:15:42   BIC                           1529.706\n",
            "Sample:                                         0   HQIC                          1519.961\n",
            "                                             - 88                                         \n",
            "Covariance Type:                              opg                                         \n",
            "==========================================================================================\n",
            "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------------\n",
            "Monthly_Festival_Index   -87.1392    222.621     -0.391      0.695    -523.469     349.191\n",
            "Temperature_Mean        3711.4880    408.517      9.085      0.000    2910.809    4512.167\n",
            "IIP_YoY                 1.243e+04   2663.996      4.667      0.000    7212.379    1.77e+04\n",
            "ar.L1                      0.5238      0.246      2.128      0.033       0.041       1.006\n",
            "ma.L1                     -0.8545      0.185     -4.617      0.000      -1.217      -0.492\n",
            "ma.S.L12                  -0.6493      0.080     -8.114      0.000      -0.806      -0.492\n",
            "sigma2                  2.702e+07      0.299   9.03e+07      0.000     2.7e+07     2.7e+07\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):                64.17\n",
            "Prob(Q):                              0.89   Prob(JB):                         0.00\n",
            "Heteroskedasticity (H):               3.13   Skew:                            -0.81\n",
            "Prob(H) (two-sided):                  0.01   Kurtosis:                         7.23\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
            "[2] Covariance matrix is singular or near-singular, with condition number 8.47e+23. Standard errors may be unstable.\n",
            "\n",
            "============================================================\n",
            "Model: SARIMAX_Validation\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         4,813.69 MU\n",
            "RMSE (Root Mean Squared Error):    5,702.15 MU\n",
            "MAPE (Mean Absolute % Error):      3.63%\n",
            "R² Score:                          0.7100\n",
            "Directional Accuracy:              88.24%\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Model: SARIMAX_Test\n",
            "============================================================\n",
            "MAE (Mean Absolute Error):         13,443.74 MU\n",
            "RMSE (Root Mean Squared Error):    16,628.95 MU\n",
            "MAPE (Mean Absolute % Error):      9.69%\n",
            "R² Score:                          -2.2719\n",
            "Directional Accuracy:              78.95%\n",
            "============================================================\n",
            "\n",
            "Saved: 09_SARIMAX_residuals.png\n",
            "\n",
            "SARIMAX model completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COMPARING MODELS"
      ],
      "metadata": {
        "id": "RZzeTbjOK7-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL COMPARISON\")\n",
        "\n",
        "# Compile all metrics\n",
        "comparison_metrics = pd.DataFrame([\n",
        "    arima_val_metrics,\n",
        "    sarima_val_metrics,\n",
        "    sarimax_val_metrics,\n",
        "    arima_test_metrics,\n",
        "    sarima_test_metrics,\n",
        "    sarimax_test_metrics\n",
        "])\n",
        "\n",
        "# Save comparison\n",
        "comparison_metrics.to_csv(f\"{FOLDERS['results_eval']}/model_comparison_metrics.csv\", index=False)\n",
        "\n",
        "print(\"\\nValidation Set Performance:\")\n",
        "print(comparison_metrics[comparison_metrics['Model'].str.contains('Validation')])\n",
        "\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(comparison_metrics[comparison_metrics['Model'].str.contains('Test')])\n",
        "\n",
        "# Visualization: Metrics Comparison (Test Set)\n",
        "test_comparison = comparison_metrics[comparison_metrics['Model'].str.contains('Test')].copy()\n",
        "test_comparison['Model'] = test_comparison['Model'].str.replace('_Test', '')\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "metrics_to_plot = ['MAE', 'RMSE', 'MAPE', 'R2_Score', 'Directional_Accuracy']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "# MAE\n",
        "axes[0, 0].bar(test_comparison['Model'], test_comparison['MAE'], color=colors)\n",
        "axes[0, 0].set_title('Mean Absolute Error (MAE)', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('MAE (MU)')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# RMSE\n",
        "axes[0, 1].bar(test_comparison['Model'], test_comparison['RMSE'], color=colors)\n",
        "axes[0, 1].set_title('Root Mean Squared Error (RMSE)', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('RMSE (MU)')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# MAPE\n",
        "axes[0, 2].bar(test_comparison['Model'], test_comparison['MAPE'], color=colors)\n",
        "axes[0, 2].set_title('Mean Absolute Percentage Error (MAPE)', fontweight='bold')\n",
        "axes[0, 2].set_ylabel('MAPE (%)')\n",
        "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# R2 Score\n",
        "axes[1, 0].bar(test_comparison['Model'], test_comparison['R2_Score'], color=colors)\n",
        "axes[1, 0].set_title('R² Score', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('R² Score')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Directional Accuracy\n",
        "axes[1, 1].bar(test_comparison['Model'], test_comparison['Directional_Accuracy'], color=colors)\n",
        "axes[1, 1].set_title('Directional Accuracy', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Summary Table\n",
        "axes[1, 2].axis('tight')\n",
        "axes[1, 2].axis('off')\n",
        "table_data = []\n",
        "for idx, row in test_comparison.iterrows():\n",
        "    table_data.append([\n",
        "        row['Model'],\n",
        "        f\"{row['MAE']:.2f}\",\n",
        "        f\"{row['RMSE']:.2f}\",\n",
        "        f\"{row['MAPE']:.2f}%\",\n",
        "        f\"{row['R2_Score']:.4f}\"\n",
        "    ])\n",
        "table = axes[1, 2].table(cellText=table_data,\n",
        "                         colLabels=['Model', 'MAE', 'RMSE', 'MAPE', 'R²'],\n",
        "                         cellLoc='center',\n",
        "                         loc='center',\n",
        "                         bbox=[0, 0, 1, 1])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1, 2)\n",
        "\n",
        "plt.suptitle('Model Performance Comparison (Test Set)', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(f\"{FOLDERS['results_eval']}/10_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"\\nSaved: 10_model_comparison.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Oca2Dv13K7ty",
        "outputId": "c6a8200f-fbb2-426a-bf96-1db61c784daf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL COMPARISON\n",
            "\n",
            "Validation Set Performance:\n",
            "                Model          MAE          RMSE      MAPE  R2_Score  Directional_Accuracy\n",
            "0    ARIMA_Validation  8761.111111  10659.005223  6.739654 -0.013402             41.176471\n",
            "1   SARIMA_Validation  4949.424004   5869.984311  3.777934  0.692658             82.352941\n",
            "2  SARIMAX_Validation  4813.691268   5702.146612  3.627113  0.709982             88.235294\n",
            "\n",
            "Test Set Performance:\n",
            "          Model           MAE          RMSE       MAPE  R2_Score  Directional_Accuracy\n",
            "3    ARIMA_Test  15055.400000  17174.748097  10.183441 -2.490251             52.631579\n",
            "4   SARIMA_Test   6215.459604   7024.210127   4.372942  0.416191             73.684211\n",
            "5  SARIMAX_Test  13443.737198  16628.947568   9.693714 -2.271940             78.947368\n",
            "\n",
            "Saved: 10_model_comparison.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACTUALvsPREDICTED"
      ],
      "metadata": {
        "id": "p9w3b5l3LBcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GENERATING ACTUAL VS PREDICTED PLOTS\")\n",
        "\n",
        "# Combine all data for plotting\n",
        "all_dates = pd.concat([train_data['Date'], val_data['Date'], test_data['Date']])\n",
        "all_actual = pd.concat([train_data['Requirement_MU'], val_data['Requirement_MU'], test_data['Requirement_MU']])\n",
        "\n",
        "# ARIMA predictions (need to re-forecast for complete series)\n",
        "arima_all_pred = np.concatenate([\n",
        "    arima_train_pred.values,\n",
        "    arima_val_pred.values,\n",
        "    arima_test_pred.values\n",
        "])\n",
        "\n",
        "# SARIMA predictions\n",
        "sarima_all_pred = np.concatenate([\n",
        "    sarima_train_pred.values,\n",
        "    sarima_val_pred.values,\n",
        "    sarima_test_pred.values\n",
        "])\n",
        "\n",
        "# SARIMAX predictions\n",
        "sarimax_all_pred = np.concatenate([\n",
        "    sarimax_train_pred.values,\n",
        "    sarimax_val_pred.values,\n",
        "    sarimax_test_pred.values\n",
        "])\n",
        "\n",
        "# Plot 1: Individual Model Predictions\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "# ARIMA\n",
        "axes[0].plot(all_dates, all_actual, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "axes[0].plot(all_dates, arima_all_pred, label='ARIMA Predicted', color='#3498db', linewidth=2, alpha=0.8)\n",
        "axes[0].axvline(x=train_data['Date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "axes[0].axvline(x=val_data['Date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "axes[0].set_title('ARIMA: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Electricity Requirement (MU)')\n",
        "axes[0].legend(loc='best')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# SARIMA\n",
        "axes[1].plot(all_dates, all_actual, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "axes[1].plot(all_dates, sarima_all_pred, label='SARIMA Predicted', color='#e74c3c', linewidth=2, alpha=0.8)\n",
        "axes[1].axvline(x=train_data['Date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].axvline(x=val_data['Date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].set_title('SARIMA: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Electricity Requirement (MU)')\n",
        "axes[1].legend(loc='best')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# SARIMAX\n",
        "axes[2].plot(all_dates, all_actual, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "axes[2].plot(all_dates, sarimax_all_pred, label='SARIMAX Predicted', color='#2ecc71', linewidth=2, alpha=0.8)\n",
        "axes[2].axvline(x=train_data['Date'].max(), color='red', linestyle='--', alpha=0.5, label='Train-Val Split')\n",
        "axes[2].axvline(x=val_data['Date'].max(), color='red', linestyle='--', alpha=0.5, label='Val-Test Split')\n",
        "axes[2].set_title('SARIMAX: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Electricity Requirement (MU)')\n",
        "axes[2].set_xlabel('Date')\n",
        "axes[2].legend(loc='best')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/11_actual_vs_predicted_individual.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 11_actual_vs_predicted_individual.png\")\n",
        "\n",
        "# Plot 2: All Models Combined\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(all_dates, all_actual, label='Actual', color='black', linewidth=2.5, alpha=0.8)\n",
        "plt.plot(all_dates, arima_all_pred, label='ARIMA', color='#3498db', linewidth=1.5, alpha=0.7, linestyle='--')\n",
        "plt.plot(all_dates, sarima_all_pred, label='SARIMA', color='#e74c3c', linewidth=1.5, alpha=0.7, linestyle='--')\n",
        "plt.plot(all_dates, sarimax_all_pred, label='SARIMAX', color='#2ecc71', linewidth=1.5, alpha=0.7, linestyle='--')\n",
        "plt.axvline(x=train_data['Date'].max(), color='gray', linestyle=':', alpha=0.5, label='Train-Val Split')\n",
        "plt.axvline(x=val_data['Date'].max(), color='gray', linestyle=':', alpha=0.5, label='Val-Test Split')\n",
        "plt.title('All Models: Actual vs Predicted Comparison', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Electricity Requirement (MU)', fontsize=12)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/12_actual_vs_predicted_all_models.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 12_actual_vs_predicted_all_models.png\")\n",
        "\n",
        "# Plot 3: Test Set Focus\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(test_data['Date'], test_data['Requirement_MU'], label='Actual',\n",
        "         color='black', linewidth=3, marker='o', markersize=5, alpha=0.8)\n",
        "plt.plot(test_data['Date'], arima_test_pred, label='ARIMA',\n",
        "         color='#3498db', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
        "plt.plot(test_data['Date'], sarima_test_pred, label='SARIMA',\n",
        "         color='#e74c3c', linewidth=2, marker='^', markersize=4, alpha=0.7)\n",
        "plt.plot(test_data['Date'], sarimax_test_pred, label='SARIMAX',\n",
        "         color='#2ecc71', linewidth=2, marker='d', markersize=4, alpha=0.7)\n",
        "plt.title('Test Set Performance: Actual vs Predicted (All Models)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Electricity Requirement (MU)', fontsize=12)\n",
        "plt.legend(loc='best', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_viz']}/13_test_set_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 13_test_set_comparison.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mxDYrySILGfR",
        "outputId": "77932aa1-6661-4df2-d98e-3b0c8a900035"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACTUAL VS PREDICTED PLOTS\n",
            "Saved: 11_actual_vs_predicted_individual.png\n",
            "Saved: 12_actual_vs_predicted_all_models.png\n",
            "Saved: 13_test_set_comparison.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FORECASTING NEXT 6MONTHS"
      ],
      "metadata": {
        "id": "awHnFpcCLMEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FUTURE FORECASTING (6 MONTHS AHEAD)\")\n",
        "\n",
        "# Generate future dates\n",
        "last_date = test_data['Date'].max()\n",
        "future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=6, freq='MS')\n",
        "future_df = pd.DataFrame({'Date': future_dates})\n",
        "future_df['Year'] = future_df['Date'].dt.year\n",
        "future_df['Month'] = future_df['Date'].dt.month\n",
        "\n",
        "print(f\"\\nForecasting for: {future_dates[0].strftime('%Y-%m')} to {future_dates[-1].strftime('%Y-%m')}\")\n",
        "\n",
        "# Prepare future exogenous variables for SARIMAX\n",
        "# Method 1: Use historical averages by month\n",
        "historical_monthly_avg = merged_df.groupby('Month')[exog_vars].mean()\n",
        "\n",
        "future_exog = pd.DataFrame()\n",
        "for idx, row in future_df.iterrows():\n",
        "    month = row['Month']\n",
        "    future_exog = pd.concat([future_exog, historical_monthly_avg.loc[[month]]], ignore_index=True)\n",
        "\n",
        "print(\"\\nFuture exogenous variables (based on historical monthly averages):\")\n",
        "print(future_exog)\n",
        "\n",
        "# ARIMA Future Forecast\n",
        "print(\"\\n[1/3] Generating ARIMA forecast...\")\n",
        "arima_future_pred = fitted_arima.forecast(steps=len(val_data) + len(test_data) + 6)\n",
        "arima_future_pred = arima_future_pred[-6:]\n",
        "\n",
        "# SARIMA Future Forecast\n",
        "print(\"[2/3] Generating SARIMA forecast...\")\n",
        "sarima_future_pred = fitted_sarima.forecast(steps=6)\n",
        "\n",
        "# SARIMAX Future Forecast\n",
        "print(\"[3/3] Generating SARIMAX forecast...\")\n",
        "sarimax_future_pred = fitted_sarimax.forecast(steps=6, exog=future_exog)\n",
        "\n",
        "# Create forecast dataframe\n",
        "forecast_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'ARIMA_Forecast': arima_future_pred.values,\n",
        "    'SARIMA_Forecast': sarima_future_pred.values,\n",
        "    'SARIMAX_Forecast': sarimax_future_pred.values\n",
        "})\n",
        "\n",
        "print(\"\\nFuture Forecasts:\")\n",
        "print(forecast_df)\n",
        "\n",
        "# Save forecasts\n",
        "forecast_df.to_csv(f\"{FOLDERS['results_forecast']}/future_forecasts_6months.csv\", index=False)\n",
        "print(f\"\\nSaved: future_forecasts_6months.csv\")\n",
        "\n",
        "# Visualize future forecasts\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(merged_df['Date'], merged_df['Requirement_MU'],\n",
        "         label='Historical Data', color='black', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot test set\n",
        "plt.plot(test_data['Date'], test_data['Requirement_MU'],\n",
        "         label='Test Set (Actual)', color='darkgray', linewidth=2.5, marker='o', markersize=5)\n",
        "\n",
        "# Plot future forecasts\n",
        "plt.plot(forecast_df['Date'], forecast_df['ARIMA_Forecast'],\n",
        "         label='ARIMA Forecast', color='#3498db', linewidth=2, marker='s', markersize=6, linestyle='--')\n",
        "plt.plot(forecast_df['Date'], forecast_df['SARIMA_Forecast'],\n",
        "         label='SARIMA Forecast', color='#e74c3c', linewidth=2, marker='^', markersize=6, linestyle='--')\n",
        "plt.plot(forecast_df['Date'], forecast_df['SARIMAX_Forecast'],\n",
        "         label='SARIMAX Forecast', color='#2ecc71', linewidth=2, marker='d', markersize=6, linestyle='--')\n",
        "\n",
        "plt.axvline(x=test_data['Date'].max(), color='red', linestyle=':', alpha=0.6, linewidth=2, label='Forecast Start')\n",
        "\n",
        "plt.title('6-Month Future Forecasts (September 2025 - February 2026)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Electricity Requirement (MU)', fontsize=12)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FOLDERS['results_forecast']}/14_future_forecasts.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved: 14_future_forecasts.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l_5IDo0TLPn-",
        "outputId": "7d5ab1f7-024d-4173-894d-bfa0475bf4ce"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FUTURE FORECASTING (6 MONTHS AHEAD)\n",
            "\n",
            "Forecasting for: 2025-09 to 2026-02\n",
            "\n",
            "Future exogenous variables (based on historical monthly averages):\n",
            "   Monthly_Festival_Index  Temperature_Mean  IIP_YoY\n",
            "0                    9.15            27.854   0.0236\n",
            "1                   14.95            26.708   0.0324\n",
            "2                    6.50            23.680   0.0274\n",
            "3                    6.00            20.830   0.0202\n",
            "4                    9.00            19.884   0.0261\n",
            "5                    1.75            22.060   0.0248\n",
            "\n",
            "[1/3] Generating ARIMA forecast...\n",
            "[2/3] Generating SARIMA forecast...\n",
            "[3/3] Generating SARIMAX forecast...\n",
            "\n",
            "Future Forecasts:\n",
            "        Date  ARIMA_Forecast  SARIMA_Forecast  SARIMAX_Forecast\n",
            "0 2025-09-01        128689.0    132288.613530     130915.809852\n",
            "1 2025-10-01        128689.0    123021.603344     120859.438237\n",
            "2 2025-11-01        128689.0    123158.274483     113349.658820\n",
            "3 2025-12-01        128689.0    111104.345808     102562.785613\n",
            "4 2026-01-01        128689.0    118853.069657     116170.619259\n",
            "5 2026-02-01        128689.0    121370.343238     131048.473105\n",
            "\n",
            "Saved: future_forecasts_6months.csv\n",
            "Saved: 14_future_forecasts.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL DIAGNOSTICS AND STATISTICAL TESTS"
      ],
      "metadata": {
        "id": "ry95CZBqLS80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"MODEL DIAGNOSTICS AND STATISTICAL TESTS\")\n",
        "\n",
        "def ljung_box_test(residuals, lags=20):\n",
        "    \"\"\"Perform Ljung-Box test on residuals\"\"\"\n",
        "    lb_test = acorr_ljungbox(residuals, lags=lags, return_df=True)\n",
        "    return lb_test\n",
        "\n",
        "# Ljung-Box Test for all models\n",
        "print(\"\\n[1/3] ARIMA Ljung-Box Test (Testing for autocorrelation in residuals):\")\n",
        "arima_lb = ljung_box_test(arima_residuals, lags=20)\n",
        "print(arima_lb.head(10))\n",
        "arima_lb.to_csv(f\"{FOLDERS['reports']}/ARIMA_ljung_box_test.csv\", index=False)\n",
        "\n",
        "print(\"\\n[2/3] SARIMA Ljung-Box Test:\")\n",
        "sarima_lb = ljung_box_test(sarima_residuals, lags=20)\n",
        "print(sarima_lb.head(10))\n",
        "sarima_lb.to_csv(f\"{FOLDERS['reports']}/SARIMA_ljung_box_test.csv\", index=False)\n",
        "\n",
        "print(\"\\n[3/3] SARIMAX Ljung-Box Test:\")\n",
        "sarimax_lb = ljung_box_test(sarimax_residuals, lags=20)\n",
        "print(sarimax_lb.head(10))\n",
        "sarimax_lb.to_csv(f\"{FOLDERS['reports']}/SARIMAX_ljung_box_test.csv\", index=False)\n",
        "\n",
        "# Normality tests\n",
        "from scipy.stats import shapiro, jarque_bera\n",
        "\n",
        "def normality_tests(residuals, model_name):\n",
        "    \"\"\"Perform normality tests on residuals\"\"\"\n",
        "    # Shapiro-Wilk test\n",
        "    shapiro_stat, shapiro_p = shapiro(residuals)\n",
        "\n",
        "    # Jarque-Bera test\n",
        "    jb_stat, jb_p = jarque_bera(residuals)\n",
        "\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Shapiro_Statistic': shapiro_stat,\n",
        "        'Shapiro_pvalue': shapiro_p,\n",
        "        'JB_Statistic': jb_stat,\n",
        "        'JB_pvalue': jb_p,\n",
        "        'Shapiro_Normal': 'Yes' if shapiro_p > 0.05 else 'No',\n",
        "        'JB_Normal': 'Yes' if jb_p > 0.05 else 'No'\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"NORMALITY TESTS ON RESIDUALS\")\n",
        "\n",
        "\n",
        "arima_norm = normality_tests(arima_residuals, 'ARIMA')\n",
        "sarima_norm = normality_tests(sarima_residuals, 'SARIMA')\n",
        "sarimax_norm = normality_tests(sarimax_residuals, 'SARIMAX')\n",
        "\n",
        "normality_df = pd.DataFrame([arima_norm, sarima_norm, sarimax_norm])\n",
        "print(\"\\n\", normality_df)\n",
        "normality_df.to_csv(f\"{FOLDERS['reports']}/normality_tests.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6zmwmEJULXvx",
        "outputId": "f710c564-b926-49c3-c0d7-33f5986a6cf0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL DIAGNOSTICS AND STATISTICAL TESTS\n",
            "\n",
            "[1/3] ARIMA Ljung-Box Test (Testing for autocorrelation in residuals):\n",
            "     lb_stat  lb_pvalue\n",
            "1   0.060256   0.806091\n",
            "2   0.204919   0.902615\n",
            "3   0.310097   0.958119\n",
            "4   1.025652   0.905882\n",
            "5   1.092471   0.954768\n",
            "6   1.670871   0.947348\n",
            "7   2.076384   0.955492\n",
            "8   2.089855   0.978133\n",
            "9   2.134621   0.989155\n",
            "10  2.153992   0.995018\n",
            "\n",
            "[2/3] SARIMA Ljung-Box Test:\n",
            "     lb_stat  lb_pvalue\n",
            "1   0.199999   0.654722\n",
            "2   0.312216   0.855467\n",
            "3   0.487086   0.921719\n",
            "4   0.649906   0.957360\n",
            "5   0.856951   0.973260\n",
            "6   0.882607   0.989674\n",
            "7   1.576782   0.979532\n",
            "8   1.867906   0.984797\n",
            "9   1.955209   0.992156\n",
            "10  1.957582   0.996655\n",
            "\n",
            "[3/3] SARIMAX Ljung-Box Test:\n",
            "     lb_stat  lb_pvalue\n",
            "1   0.453729   0.500570\n",
            "2   0.573761   0.750602\n",
            "3   3.486978   0.322455\n",
            "4   3.624772   0.459162\n",
            "5   3.741973   0.587130\n",
            "6   3.783391   0.705962\n",
            "7   4.172822   0.759671\n",
            "8   4.341046   0.825117\n",
            "9   4.495147   0.875914\n",
            "10  4.535092   0.919998\n",
            "NORMALITY TESTS ON RESIDUALS\n",
            "\n",
            "      Model  Shapiro_Statistic  Shapiro_pvalue  JB_Statistic     JB_pvalue Shapiro_Normal JB_Normal\n",
            "0    ARIMA           0.641422    2.431980e-13   4023.510667  0.000000e+00             No        No\n",
            "1   SARIMA           0.609942    6.108466e-14   2341.371009  0.000000e+00             No        No\n",
            "2  SARIMAX           0.943595    8.030511e-04     35.051665  2.446964e-08             No        No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REPORT"
      ],
      "metadata": {
        "id": "Qm9JEt4ELXZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"GENERATING COMPREHENSIVE SUMMARY REPORT\")\n",
        "\n",
        "\n",
        "summary_report = f\"\"\"\n",
        "{'_'*80}\n",
        "TIME SERIES FORECASTING PROJECT - COMPREHENSIVE SUMMARY REPORT\n",
        "{'_'*80}\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Author: JIMS Rohini Student\n",
        "Project: Electricity Demand Forecasting for India\n",
        "\n",
        "{'_'*80}\n",
        "1. DATASET INFORMATION\n",
        "{'_'*80}\n",
        "\n",
        "Target Variable: Electricity Requirement (MU)\n",
        "Date Range: April 2015 to August 2025\n",
        "Total Observations: {len(merged_df)}\n",
        "\n",
        "Exogenous Variables (SARIMAX):\n",
        "  - Monthly Festival Index\n",
        "  - Temperature Mean (°C)\n",
        "  - IIP Year-over-Year Growth (%)\n",
        "\n",
        "Data Split:\n",
        "  - Training Set: {len(train_data)} months ({len(train_data)/len(merged_df)*100:.1f}%)\n",
        "    Range: {train_data['Date'].min().strftime('%Y-%m')} to {train_data['Date'].max().strftime('%Y-%m')}\n",
        "\n",
        "  - Validation Set: {len(val_data)} months ({len(val_data)/len(merged_df)*100:.1f}%)\n",
        "    Range: {val_data['Date'].min().strftime('%Y-%m')} to {val_data['Date'].max().strftime('%Y-%m')}\n",
        "\n",
        "  - Test Set: {len(test_data)} months ({len(test_data)/len(merged_df)*100:.1f}%)\n",
        "    Range: {test_data['Date'].min().strftime('%Y-%m')} to {test_data['Date'].max().strftime('%Y-%m')}\n",
        "\n",
        "{'_'*80}\n",
        "2. STATIONARITY ANALYSIS\n",
        "{'_'*80}\n",
        "\n",
        "Augmented Dickey-Fuller (ADF) Test:\n",
        "  Test Statistic: {adf_results['Test Statistic']:.6f}\n",
        "  p-value: {adf_results['p-value']:.6f}\n",
        "  Result: {'STATIONARY' if adf_results['p-value'] < 0.05 else 'NON-STATIONARY'}\n",
        "\n",
        "KPSS Test:\n",
        "  Test Statistic: {kpss_results['Test Statistic']:.6f}\n",
        "  p-value: {kpss_results['p-value']:.6f}\n",
        "  Result: {'STATIONARY' if kpss_results['p-value'] >= 0.05 else 'NON-STATIONARY'}\n",
        "\n",
        "{'_'*80}\n",
        "3. MODEL SPECIFICATIONS\n",
        "{'_'*80}\n",
        "\n",
        "MODEL 1: ARIMA\n",
        "  Parameters: {best_params}\n",
        "  AIC: {best_aic:.2f}\n",
        "  Parameter Selection: Grid Search\n",
        "\n",
        "MODEL 2: SARIMA\n",
        "  Order (p,d,q): {best_params_sarima[0]}\n",
        "  Seasonal (P,D,Q,m): {best_params_sarima[1]}\n",
        "  AIC: {best_aic_sarima:.2f}\n",
        "  Parameter Selection: Grid Search\n",
        "\n",
        "MODEL 3: SARIMAX\n",
        "  Order (p,d,q): {best_params_sarimax[0]}\n",
        "  Seasonal (P,D,Q,m): {best_params_sarimax[1]}\n",
        "  Exogenous Variables: 3 (Festival Index, Temperature, IIP)\n",
        "  AIC: {best_aic_sarimax:.2f}\n",
        "  Parameter Selection: Grid Search\n",
        "\n",
        "{'_'*80}\n",
        "4. VALIDATION SET PERFORMANCE\n",
        "{'_'*80}\n",
        "\n",
        "ARIMA:\n",
        "  MAE:  {arima_val_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {arima_val_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {arima_val_metrics['MAPE']:.2f}%\n",
        "  R²:   {arima_val_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {arima_val_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "SARIMA:\n",
        "  MAE:  {sarima_val_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {sarima_val_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {sarima_val_metrics['MAPE']:.2f}%\n",
        "  R²:   {sarima_val_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {sarima_val_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "SARIMAX:\n",
        "  MAE:  {sarimax_val_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {sarimax_val_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {sarimax_val_metrics['MAPE']:.2f}%\n",
        "  R²:   {sarimax_val_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {sarimax_val_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "{'_'*80}\n",
        "5. TEST SET PERFORMANCE (FINAL EVALUATION)\n",
        "{'_'*80}\n",
        "\n",
        "ARIMA:\n",
        "  MAE:  {arima_test_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {arima_test_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {arima_test_metrics['MAPE']:.2f}%\n",
        "  R²:   {arima_test_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {arima_test_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "SARIMA:\n",
        "  MAE:  {sarima_test_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {sarima_test_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {sarima_test_metrics['MAPE']:.2f}%\n",
        "  R²:   {sarima_test_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {sarima_test_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "SARIMAX:\n",
        "  MAE:  {sarimax_test_metrics['MAE']:,.2f} MU\n",
        "  RMSE: {sarimax_test_metrics['RMSE']:,.2f} MU\n",
        "  MAPE: {sarimax_test_metrics['MAPE']:.2f}%\n",
        "  R²:   {sarimax_test_metrics['R2_Score']:.4f}\n",
        "  Directional Accuracy: {sarimax_test_metrics['Directional_Accuracy']:.2f}%\n",
        "\n",
        "{'_'*80}\n",
        "6. BEST MODEL SELECTION\n",
        "{'_'*80}\n",
        "\n",
        "Based on Test Set Performance:\n",
        "\n",
        "Best MAE:  {test_comparison.loc[test_comparison['MAE'].idxmin(), 'Model']} ({test_comparison['MAE'].min():,.2f} MU)\n",
        "Best RMSE: {test_comparison.loc[test_comparison['RMSE'].idxmin(), 'Model']} ({test_comparison['RMSE'].min():,.2f} MU)\n",
        "Best MAPE: {test_comparison.loc[test_comparison['MAPE'].idxmin(), 'Model']} ({test_comparison['MAPE'].min():.2f}%)\n",
        "Best R²:   {test_comparison.loc[test_comparison['R2_Score'].idxmax(), 'Model']} ({test_comparison['R2_Score'].max():.4f})\n",
        "Best Directional Accuracy: {test_comparison.loc[test_comparison['Directional_Accuracy'].idxmax(), 'Model']} ({test_comparison['Directional_Accuracy'].max():.2f}%)\n",
        "\n",
        "{'_'*80}\n",
        "7. FUTURE FORECASTS (6 MONTHS AHEAD)\n",
        "{'_'*80}\n",
        "\n",
        "Forecast Period: September 2025 - February 2026\n",
        "\n",
        "{forecast_df.to_string(index=False)}\n",
        "\n",
        "{'_'*80}\n",
        "8. MODEL DIAGNOSTICS\n",
        "{'_'*80}\n",
        "\n",
        "Ljung-Box Test Results (p-values < 0.05 indicate autocorrelation):\n",
        "  ARIMA:   See detailed results in ARIMA_ljung_box_test.csv\n",
        "  SARIMA:  See detailed results in SARIMA_ljung_box_test.csv\n",
        "  SARIMAX: See detailed results in SARIMAX_ljung_box_test.csv\n",
        "\n",
        "Normality Tests:\n",
        "  ARIMA Residuals:   Shapiro p-value = {arima_norm['Shapiro_pvalue']:.6f} ({'Normal' if arima_norm['Shapiro_Normal']=='Yes' else 'Non-normal'})\n",
        "  SARIMA Residuals:  Shapiro p-value = {sarima_norm['Shapiro_pvalue']:.6f} ({'Normal' if sarima_norm['Shapiro_Normal']=='Yes' else 'Non-normal'})\n",
        "  SARIMAX Residuals: Shapiro p-value = {sarimax_norm['Shapiro_pvalue']:.6f} ({'Normal' if sarimax_norm['Shapiro_Normal']=='Yes' else 'Non-normal'})\n",
        "\n",
        "{'_'*80}\n",
        "9. KEY INSIGHTS\n",
        "{'_'*80}\n",
        "\n",
        "1. Seasonal Pattern: The data shows strong seasonality with period = 12 months\n",
        "2. Trend: {'Upward' if decomposition.trend[-1] > decomposition.trend[0] else 'Downward'} trend observed\n",
        "3. Exogenous Variables Impact: SARIMAX incorporates festival patterns, temperature,\n",
        "   and industrial production which affect electricity demand\n",
        "4. Best Performer: {test_comparison.loc[test_comparison['MAE'].idxmin(), 'Model']} achieved lowest prediction error\n",
        "\n",
        "{'_'*80}\n",
        "10. FILES GENERATED\n",
        "{'_'*80}\n",
        "\n",
        "Data Files:\n",
        "  - merged_data.csv (Cleaned and merged dataset)\n",
        "  - future_forecasts_6months.csv (6-month ahead forecasts)\n",
        "\n",
        "Model Reports:\n",
        "  - ARIMA_model_summary.txt\n",
        "  - SARIMA_model_summary.txt\n",
        "  - SARIMAX_model_summary.txt\n",
        "  - ARIMA_grid_search_results.csv\n",
        "  - SARIMA_grid_search_results.csv\n",
        "  - SARIMAX_grid_search_results.csv\n",
        "\n",
        "Evaluations:\n",
        "  - model_comparison_metrics.csv\n",
        "  - descriptive_statistics.csv\n",
        "  - correlation_matrix.csv\n",
        "  - stationarity_tests.csv\n",
        "  - normality_tests.csv\n",
        "  - seasonal_decomposition.csv\n",
        "  - ARIMA_ljung_box_test.csv\n",
        "  - SARIMA_ljung_box_test.csv\n",
        "  - SARIMAX_ljung_box_test.csv\n",
        "\n",
        "Visualizations:\n",
        "  - 01_time_series_overview.png\n",
        "  - 02_correlation_matrix.png\n",
        "  - 03_distributions.png\n",
        "  - 04_seasonal_decomposition.png\n",
        "  - 05_train_val_test_split.png\n",
        "  - 06_acf_pacf_plots.png\n",
        "  - 07_ARIMA_residuals.png\n",
        "  - 08_SARIMA_residuals.png\n",
        "  - 09_SARIMAX_residuals.png\n",
        "  - 10_model_comparison.png\n",
        "  - 11_actual_vs_predicted_individual.png\n",
        "  - 12_actual_vs_predicted_all_models.png\n",
        "  - 13_test_set_comparison.png\n",
        "  - 14_future_forecasts.png\n",
        "\n",
        "{'_'*80}\n",
        "END OF REPORT\n",
        "{'_'*80}\n",
        "\"\"\"\n",
        "\n",
        "# Save summary report\n",
        "with open(f\"{FOLDERS['reports']}/COMPREHENSIVE_SUMMARY_REPORT.txt\", 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(summary_report)\n",
        "print(f\"\\nSaved: COMPREHENSIVE_SUMMARY_REPORT.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JXoQhdfkLaW9",
        "outputId": "8b7bbc44-3b54-4f1d-f14f-585c84281243"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING COMPREHENSIVE SUMMARY REPORT\n",
            "\n",
            "________________________________________________________________________________\n",
            "TIME SERIES FORECASTING PROJECT - COMPREHENSIVE SUMMARY REPORT\n",
            "________________________________________________________________________________\n",
            "Generated: 2025-11-20 18:15:53\n",
            "Author: JIMS Rohini Student\n",
            "Project: Electricity Demand Forecasting for India\n",
            "\n",
            "________________________________________________________________________________\n",
            "1. DATASET INFORMATION\n",
            "________________________________________________________________________________\n",
            "\n",
            "Target Variable: Electricity Requirement (MU)\n",
            "Date Range: April 2015 to August 2025\n",
            "Total Observations: 126\n",
            "\n",
            "Exogenous Variables (SARIMAX):\n",
            "  - Monthly Festival Index\n",
            "  - Temperature Mean (°C)\n",
            "  - IIP Year-over-Year Growth (%)\n",
            "\n",
            "Data Split:\n",
            "  - Training Set: 88 months (69.8%)\n",
            "    Range: 2015-04 to 2022-07\n",
            "\n",
            "  - Validation Set: 18 months (14.3%)\n",
            "    Range: 2022-08 to 2024-01\n",
            "\n",
            "  - Test Set: 20 months (15.9%)\n",
            "    Range: 2024-02 to 2025-08\n",
            "\n",
            "________________________________________________________________________________\n",
            "2. STATIONARITY ANALYSIS\n",
            "________________________________________________________________________________\n",
            "\n",
            "Augmented Dickey-Fuller (ADF) Test:\n",
            "  Test Statistic: 0.766288\n",
            "  p-value: 0.991072\n",
            "  Result: NON-STATIONARY\n",
            "\n",
            "KPSS Test:\n",
            "  Test Statistic: 1.682922\n",
            "  p-value: 0.010000\n",
            "  Result: NON-STATIONARY\n",
            "\n",
            "________________________________________________________________________________\n",
            "3. MODEL SPECIFICATIONS\n",
            "________________________________________________________________________________\n",
            "\n",
            "MODEL 1: ARIMA\n",
            "  Parameters: (0, 1, 0)\n",
            "  AIC: 1795.12\n",
            "  Parameter Selection: Grid Search\n",
            "\n",
            "MODEL 2: SARIMA\n",
            "  Order (p,d,q): (0, 1, 2)\n",
            "  Seasonal (P,D,Q,m): (1, 1, 1, 12)\n",
            "  AIC: 1547.44\n",
            "  Parameter Selection: Grid Search\n",
            "\n",
            "MODEL 3: SARIMAX\n",
            "  Order (p,d,q): (1, 1, 1)\n",
            "  Seasonal (P,D,Q,m): (0, 1, 1, 12)\n",
            "  Exogenous Variables: 3 (Festival Index, Temperature, IIP)\n",
            "  AIC: 1513.48\n",
            "  Parameter Selection: Grid Search\n",
            "\n",
            "________________________________________________________________________________\n",
            "4. VALIDATION SET PERFORMANCE\n",
            "________________________________________________________________________________\n",
            "\n",
            "ARIMA:\n",
            "  MAE:  8,761.11 MU\n",
            "  RMSE: 10,659.01 MU\n",
            "  MAPE: 6.74%\n",
            "  R²:   -0.0134\n",
            "  Directional Accuracy: 41.18%\n",
            "\n",
            "SARIMA:\n",
            "  MAE:  4,949.42 MU\n",
            "  RMSE: 5,869.98 MU\n",
            "  MAPE: 3.78%\n",
            "  R²:   0.6927\n",
            "  Directional Accuracy: 82.35%\n",
            "\n",
            "SARIMAX:\n",
            "  MAE:  4,813.69 MU\n",
            "  RMSE: 5,702.15 MU\n",
            "  MAPE: 3.63%\n",
            "  R²:   0.7100\n",
            "  Directional Accuracy: 88.24%\n",
            "\n",
            "________________________________________________________________________________\n",
            "5. TEST SET PERFORMANCE (FINAL EVALUATION)\n",
            "________________________________________________________________________________\n",
            "\n",
            "ARIMA:\n",
            "  MAE:  15,055.40 MU\n",
            "  RMSE: 17,174.75 MU\n",
            "  MAPE: 10.18%\n",
            "  R²:   -2.4903\n",
            "  Directional Accuracy: 52.63%\n",
            "\n",
            "SARIMA:\n",
            "  MAE:  6,215.46 MU\n",
            "  RMSE: 7,024.21 MU\n",
            "  MAPE: 4.37%\n",
            "  R²:   0.4162\n",
            "  Directional Accuracy: 73.68%\n",
            "\n",
            "SARIMAX:\n",
            "  MAE:  13,443.74 MU\n",
            "  RMSE: 16,628.95 MU\n",
            "  MAPE: 9.69%\n",
            "  R²:   -2.2719\n",
            "  Directional Accuracy: 78.95%\n",
            "\n",
            "________________________________________________________________________________\n",
            "6. BEST MODEL SELECTION\n",
            "________________________________________________________________________________\n",
            "\n",
            "Based on Test Set Performance:\n",
            "\n",
            "Best MAE:  SARIMA (6,215.46 MU)\n",
            "Best RMSE: SARIMA (7,024.21 MU)\n",
            "Best MAPE: SARIMA (4.37%)\n",
            "Best R²:   SARIMA (0.4162)\n",
            "Best Directional Accuracy: SARIMAX (78.95%)\n",
            "\n",
            "________________________________________________________________________________\n",
            "7. FUTURE FORECASTS (6 MONTHS AHEAD)\n",
            "________________________________________________________________________________\n",
            "\n",
            "Forecast Period: September 2025 - February 2026\n",
            "\n",
            "      Date  ARIMA_Forecast  SARIMA_Forecast  SARIMAX_Forecast\n",
            "2025-09-01        128689.0    132288.613530     130915.809852\n",
            "2025-10-01        128689.0    123021.603344     120859.438237\n",
            "2025-11-01        128689.0    123158.274483     113349.658820\n",
            "2025-12-01        128689.0    111104.345808     102562.785613\n",
            "2026-01-01        128689.0    118853.069657     116170.619259\n",
            "2026-02-01        128689.0    121370.343238     131048.473105\n",
            "\n",
            "________________________________________________________________________________\n",
            "8. MODEL DIAGNOSTICS\n",
            "________________________________________________________________________________\n",
            "\n",
            "Ljung-Box Test Results (p-values < 0.05 indicate autocorrelation):\n",
            "  ARIMA:   See detailed results in ARIMA_ljung_box_test.csv\n",
            "  SARIMA:  See detailed results in SARIMA_ljung_box_test.csv\n",
            "  SARIMAX: See detailed results in SARIMAX_ljung_box_test.csv\n",
            "\n",
            "Normality Tests:\n",
            "  ARIMA Residuals:   Shapiro p-value = 0.000000 (Non-normal)\n",
            "  SARIMA Residuals:  Shapiro p-value = 0.000000 (Non-normal)\n",
            "  SARIMAX Residuals: Shapiro p-value = 0.000803 (Non-normal)\n",
            "\n",
            "________________________________________________________________________________\n",
            "9. KEY INSIGHTS\n",
            "________________________________________________________________________________\n",
            "\n",
            "1. Seasonal Pattern: The data shows strong seasonality with period = 12 months\n",
            "2. Trend: Downward trend observed\n",
            "3. Exogenous Variables Impact: SARIMAX incorporates festival patterns, temperature,\n",
            "   and industrial production which affect electricity demand\n",
            "4. Best Performer: SARIMA achieved lowest prediction error\n",
            "\n",
            "________________________________________________________________________________\n",
            "10. FILES GENERATED\n",
            "________________________________________________________________________________\n",
            "\n",
            "Data Files:\n",
            "  - merged_data.csv (Cleaned and merged dataset)\n",
            "  - future_forecasts_6months.csv (6-month ahead forecasts)\n",
            "\n",
            "Model Reports:\n",
            "  - ARIMA_model_summary.txt\n",
            "  - SARIMA_model_summary.txt\n",
            "  - SARIMAX_model_summary.txt\n",
            "  - ARIMA_grid_search_results.csv\n",
            "  - SARIMA_grid_search_results.csv\n",
            "  - SARIMAX_grid_search_results.csv\n",
            "\n",
            "Evaluations:\n",
            "  - model_comparison_metrics.csv\n",
            "  - descriptive_statistics.csv\n",
            "  - correlation_matrix.csv\n",
            "  - stationarity_tests.csv\n",
            "  - normality_tests.csv\n",
            "  - seasonal_decomposition.csv\n",
            "  - ARIMA_ljung_box_test.csv\n",
            "  - SARIMA_ljung_box_test.csv\n",
            "  - SARIMAX_ljung_box_test.csv\n",
            "\n",
            "Visualizations:\n",
            "  - 01_time_series_overview.png\n",
            "  - 02_correlation_matrix.png\n",
            "  - 03_distributions.png\n",
            "  - 04_seasonal_decomposition.png\n",
            "  - 05_train_val_test_split.png\n",
            "  - 06_acf_pacf_plots.png\n",
            "  - 07_ARIMA_residuals.png\n",
            "  - 08_SARIMA_residuals.png\n",
            "  - 09_SARIMAX_residuals.png\n",
            "  - 10_model_comparison.png\n",
            "  - 11_actual_vs_predicted_individual.png\n",
            "  - 12_actual_vs_predicted_all_models.png\n",
            "  - 13_test_set_comparison.png\n",
            "  - 14_future_forecasts.png\n",
            "\n",
            "________________________________________________________________________________\n",
            "END OF REPORT\n",
            "________________________________________________________________________________\n",
            "\n",
            "\n",
            "Saved: COMPREHENSIVE_SUMMARY_REPORT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CREATE MODEL PREDICTIONS CSV FILES"
      ],
      "metadata": {
        "id": "siClAyYNMxwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"SAVING DETAILED PREDICTIONS\")\n",
        "\n",
        "\n",
        "# Create detailed predictions dataframe\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Date': all_dates.values,\n",
        "    'Actual': all_actual.values,\n",
        "    'ARIMA_Predicted': arima_all_pred,\n",
        "    'SARIMA_Predicted': sarima_all_pred,\n",
        "    'SARIMAX_Predicted': sarimax_all_pred,\n",
        "    'ARIMA_Error': all_actual.values - arima_all_pred,\n",
        "    'SARIMA_Error': all_actual.values - sarima_all_pred,\n",
        "    'SARIMAX_Error': all_actual.values - sarimax_all_pred\n",
        "})\n",
        "\n",
        "# Add split indicator\n",
        "predictions_df['Data_Split'] = 'Train'\n",
        "predictions_df.loc[predictions_df['Date'] >= val_data['Date'].min(), 'Data_Split'] = 'Validation'\n",
        "predictions_df.loc[predictions_df['Date'] >= test_data['Date'].min(), 'Data_Split'] = 'Test'\n",
        "\n",
        "predictions_df.to_csv(f\"{FOLDERS['results_forecast']}/all_predictions_detailed.csv\", index=False)\n",
        "print(\"Saved: all_predictions_detailed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fg3LvmQ0N7Ha",
        "outputId": "94f61dd7-f921-40fb-bdc7-866dd0a41d97"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVING DETAILED PREDICTIONS\n",
            "Saved: all_predictions_detailed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FINAL SUMMARY"
      ],
      "metadata": {
        "id": "v4wJTc0tMrrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PROJECT EXECUTION COMPLETEDDDD\")\n",
        "\n",
        "execution_end_time = datetime.now()\n",
        "print(f\"\\nExecution completed at: {execution_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "\n",
        "print(\"ALL FILES SAVED TO GOOGLE DRIVE\")\n",
        "\n",
        "print(f\"\\nBase Directory: {BASE_PATH}\")\n",
        "print(\"\\nFolder Structure:\")\n",
        "for folder_name, folder_path in FOLDERS.items():\n",
        "    print(f\"  - {folder_name}: {folder_path}\")\n",
        "\n",
        "\n",
        "print(\"NEXT STEPS\")\n",
        "\n",
        "print(\"\"\"\n",
        "1. Review the COMPREHENSIVE_SUMMARY_REPORT.txt for detailed analysis\n",
        "2. Check visualizations in results/visualizations/\n",
        "3. Review model performance metrics in results/evaluations/\n",
        "4. Examine future forecasts in results/forecasts/\n",
        "5. Use model diagnostics for your project report\n",
        "\n",
        "All datasets, graphs, and evaluations have been exported to your Google Drive.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sRuk-uiPMnAV",
        "outputId": "9e87f32d-4c65-40c7-def5-b07c9c7ef7b7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT EXECUTION COMPLETEDDDD\n",
            "\n",
            "Execution completed at: 2025-11-20 18:15:53\n",
            "ALL FILES SAVED TO GOOGLE DRIVE\n",
            "\n",
            "Base Directory: /content/drive/MyDrive/Time_Series_Project\n",
            "\n",
            "Folder Structure:\n",
            "  - data_raw: /content/drive/MyDrive/Time_Series_Project/data/raw\n",
            "  - data_cleaned: /content/drive/MyDrive/Time_Series_Project/data/cleaned\n",
            "  - models: /content/drive/MyDrive/Time_Series_Project/models\n",
            "  - results_viz: /content/drive/MyDrive/Time_Series_Project/results/visualizations\n",
            "  - results_eval: /content/drive/MyDrive/Time_Series_Project/results/evaluations\n",
            "  - results_forecast: /content/drive/MyDrive/Time_Series_Project/results/forecasts\n",
            "  - reports: /content/drive/MyDrive/Time_Series_Project/reports\n",
            "NEXT STEPS\n",
            "\n",
            "1. Review the COMPREHENSIVE_SUMMARY_REPORT.txt for detailed analysis\n",
            "2. Check visualizations in results/visualizations/\n",
            "3. Review model performance metrics in results/evaluations/\n",
            "4. Examine future forecasts in results/forecasts/\n",
            "5. Use model diagnostics for your project report\n",
            "\n",
            "All datasets, graphs, and evaluations have been exported to your Google Drive.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}